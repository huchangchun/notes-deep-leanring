{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、神经元是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人工神经网络里的神经元是什么，有什么用，只有弄清楚这个问题，你才知道你在哪里。在做什么。要往哪走。\n",
    "\n",
    "首先看一下神经元的结构，看下图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/netuit.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先忽略激活函数不管：\n",
    "- 输入： $X_{1},X_{2},\\cdots ,X_{n}$  $\\space\\space\\space\\space$ (1.1)\n",
    "- 输出：$y$                $\\space\\space\\space\\space$ (1.2)\n",
    "- 输入和输出的关系（函数）：\n",
    " \n",
    " $$\n",
    "  y = (X_{1}\\times W_{1} +X_{2}\\times W_{2} + \\cdots +X_{n}\\times W_{n}) + \n",
    "  b \\space\\space\\space\\space（1.3）$$\n",
    " \n",
    "其中，$W_{i}$,$i=1$,$n$为权重（weight）,待会就知道权重为何物\n",
    "    \n",
    " \n",
    " 因为我们的数据都是离散的，为了看的更清楚，我们换个表达式把数据以向量的形式表示出来：\n",
    " - 改写输入：\n",
    "     $$\n",
    "         X=[X_{1},X_{2},\\cdots,X_{n}]^T  ，X看成n维的向量 \\space\\space\\space\\space  (1.4)\n",
    "         $$ \n",
    "      \n",
    " - 改写权重：\n",
    "$$\n",
    "W =\\begin{bmatrix}W_{1} & W_{2}&\\cdots &W_{n}\\end{bmatrix}，W看成是一行n列的矩阵  \\space\\space\\space\\space(1.5)\n",
    "$$\n",
    " - 改写y:\n",
    "$$\n",
    "y = \\begin{bmatrix}W_{1} & W_{2}&\\cdots &W_{n}\\end{bmatrix} \\cdot  \\begin{bmatrix}X_{1}\\\\X_{2}\\\\\\vdots\\\\X_{n}\\end{bmatrix} + b \n",
    "\\space\\space\\space\\space(1.6) $$,这不就是线性回归模型吗？可以看到$W$就是斜率啊\n",
    "\n",
    "现在回答刚才的问题：\n",
    "- 一个神经元是什么：参照式（1.6），从函数图像角度看，这就是一条直线。\n",
    "- 一个神经元有什么作用：要说明用途就要给出一个应用场景：分类。一个神经元就是一条直线，此时它就是个分类器。所以在线性场景下，\n",
    "单个神经元能达到分类的作用，它总能学习到一条合适的直线，将两类元素区分出来。看图说话 http://t.cn/RBCoWof\n",
    "如下图：\n",
    "\n",
    "![bp2](./data/BP2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "对上图简要说明：\n",
    "我们需要对神经元的输出做判定，那么久需要判定规则，通过判定规则后我们才能拿到我们想要的结果，这个规则是：\n",
    "\n",
    "1、假设，0代表红点，1代表蓝点（这些数据都是事先标定好的，在监督学习下，神经元会知道点是什么颜色并以这个已知结果作为标杆进行学习）\n",
    "2、当神经元输出小于等于0时，最终结果输出为0，这是个红点\n",
    "3、当神经元输出大于1时，最终结果输出为1，这是个蓝点。\n",
    "\n",
    "上面的规则有点激活函数的味道（这里只是线性场景，虽然不适合，但是简单期间，使用了单位跃阶函数来描述激活函数的功能）当x<=0时，y=0,当x>0时，y=1）\n",
    "单位跃阶函数：\n",
    "![bp02](./data/bp02.jpg)\n",
    "此时神经元的长相：\n",
    "![bp03](./data/bp03.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、激活函数是什么，有什么用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的例子，其实已经说明了激活函数的作用（线性模型的表达能力不够，引入激活函数是为了添加非线性因素），但是我们通常面临的问题，不是简单的线性问题，不能用单位跃阶函数作为激活函数，原因是\n",
    "跃阶函数在x=0时不连续，即不可导，在非0处导数为0，（说人话）就是它具备输出限定在$[0-1]$,但是它不具备丝滑的特性，这个特性很重要。并且在非0处导数为0，\n",
    "也就是硬饱和，压根儿就没有梯度可言，梯度也很重要，梯度意味着在神经元传播间是由反应的，而不是“死了的”。\n",
    "\n",
    " 那激活函数需要具备什么特性呢：\n",
    "- 非线性：即导数不是常熟，不然就退化成直线，对于一些画一条直线仍然无法分开的问题，非线性可以把直线掰弯，自从掰弯，自从变弯以后就保罗万象了。\n",
    "- 几乎处处可导： 也就是具备“丝滑的特性”,数学上处处可导为后面讲到的BP提供了核心条件\n",
    "- 输出范围有限：一般是限定在[0,1]有限的输出范围使得神经元对于一些比较大的输入也会比较稳定。\n",
    "- 非饱和性：饱和就是指，当输入比较大的时候，几乎没变化了，那么会导致梯度消失！什么是梯度消失：就是输出慢慢的没有变化了。梯度消失带来的负面影响就是限制了神经网络表达能力，词穷的感觉你有么。sigmod,tanh函数都是软饱和的，跃阶函数是硬饱和的。软饱和是指输入趋于无穷大的时候输出无限接近上限，硬是指像跃阶函数那样，输入非0输出就已经始终都是上限值。\n",
    "如果激活函数是饱和的，带来的缺陷就是系统迭代更新变慢，系统收敛就慢，当然这是可以有办法弥补的，一种方法就是使用交叉熵函数作为损失函数，Relu是非饱和的，亲测效果挺不错\n",
    "- 单调性：即导数符号不变，导数要么一直大于0，要么一直小于0，不要上窜下跳。导数符号不变，让神经网络训练容易收敛。\n",
    "\n",
    "- 更多参考[04-激活函数](04-激活函数.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们将用到的激活函数：\n",
    "Sigmod函数：$y = \\frac{1}{1+e^{-x}}   \\space\\space\\space\\space  (2.1)$    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYXaz4eQemOIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111c6c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-10,10,0.1)\n",
    "y = [sigmoid(i) for i in x]\n",
    "plt.figure()\n",
    "plt.plot(x,y)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对Sigmod函数求导（后面会用到）：\n",
    "首先要了解复合函数的求导：\n",
    "- 法则1：$[u(x) \\pm v(x)] = u(x) \\pm v(x)  \\space\\space\\space\\space  (2.2)$             \n",
    "- 法则2：$[u(x)  \\ast v(x)] = u^{'}(x)  \\ast v(x) +u(x)  \\ast v^{'}(x)\\space\\space\\space\\space  (2.3)$\n",
    "- 法则3：$\\frac{u}{v}^{'} = \\frac{u^{'}v-uv^{'}}{v^{2}}\\space\\space\\space\\space  (2.4)$ \n",
    "\n",
    "开始对Sigmoid求导：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "y =& \\left(\\frac{1}{1+\\space e^{-x}}^{'}\\right)\\\\\n",
    "=&\\left(\\frac{u}{v}^{'}\\right)  \\space\\space\\space\\space  (设u=1,v = 1+e^{-x})\\\\\n",
    "=&\\frac{u^{'}v-uv^{'}}{v^{2}}\\\\\n",
    "=&\\frac{1^{'} \\ast (e^{-x} \\space + \\space 1)\\space-1 \\ast (e^{-x}\\space + \\space 1)^{'}}{(e^{-x} \\space + \\space 1)^{2}}\\\\\n",
    "=&\\frac{e^{-x}}{(e^{-x} \\space + \\space 1)^{2}}\\\\\n",
    "=&\\frac{1}{(e^{-x} \\space + \\space 1)} \\ast \\frac{e^{-x}\\space+\\space1-\\space 1}{(e^{-x} \\space + \\space 1)}\\\\\n",
    "=&\\frac{1}{(e^{-x} \\space + \\space 1)} \\ast \\left(1- \\frac{1}{(e^{-x} \\space + \\space 1)}\\right)\\\\\n",
    "=&y \\ast (1-y)\\end{aligned}    \\space \\space \\space \\space (2.5)$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/x+pLzsoEyjJ6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1162ca90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#导数图像为：\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "def gety(x):\n",
    "    return np.exp(-x)/np.power((1+np.exp(-x)),2)\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-10,10,0.1)\n",
    "y = [gety(i) for i in x]\n",
    "plt.figure()\n",
    "plt.plot(x,y)\n",
    "plt.ylabel('Y Axis')\n",
    "plt.xlabel('X Axis')\n",
    "plt.title(\"Derivative of Sigmoid Activation Function\")\n",
    "plt.text(1.5,0.15,r'$\\sigma(x) \\ast (1-\\sigma(x))$')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、BP（Back Progagation）算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.神经元的结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过上面的介绍，我们知道单个神经元只能处理线性可分情况，唯有多个神经元组成的网络才能处理更复杂的问题。神经网络是一种分层结构，一般\n",
    "由输入层、隐藏层，输出层组成，所以神经网络至少有3层，隐藏层多于1，总层数大于3的就是我们所说的深度学习。\n",
    "- 输入层：就是接收原始数据，然后往隐藏层送\n",
    "- 输出层：神经网络的决策输出\n",
    "- 隐藏层：该层是神经网络的关键，相当于对数据做一次特征提取，隐藏层的意义是把前一层的向量变成新的向量，就是坐标变换通过$ \\alpha(W.\\overrightarrow{x}+b)$把数据做平移、旋转、伸缩、扭曲 [参考02-why-Deep](02-why-Deep.ipynb)，这样让数据变得线性可分。\n",
    "可以说：神经网络换着坐标空间玩数据，根据需要，可降维，可升维，可大，可小，可圆，可扁，就是这么“无敌”\n",
    "这个可以自己玩玩，直观的感受一下：[传送门](https://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.正向传播过程\n",
    "看图，这是一个典型的三层神经网络结构，第一层是输入层，第二层是隐藏层，第三层是输出层。PS:不同的应用场景，神经网络的结构要有针对性的设计，这里仅仅是为了推导算法和计算方便才采用这个简单的结构\n",
    "![0404](./data/04-04.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们以战士打靶，目标是训练战士能命中靶心成为神枪手作为场景：\n",
    "那么我们手里有这样一些数据：一堆枪摆放的位置(x,y)，以及射击结果，命中靶心和不命中靶心。\n",
    "- 我们的目标是：训练出一个神经网络模型，输入一个点的坐标（射击姿势），它就告诉你这个点是什么结果（是否命中）。-\n",
    "- 我们的方法是：训练一个能根据误差不断自我调整的模型，训练模型的步骤是：\n",
    "\n",
    " - 正向传播：把点的坐标数据输入神经网络，然后开始一层一层的传播下去，直到输出层输出结果。\n",
    " - 反向传播(BP)：就好比战士去靶场打靶，枪的摆放位置（输入），和靶心（期望的输出）是已知。战士（神经网络）一开始的时候是这样做的，\n",
    " 随便开一枪$（w，b参数初始化称随机值,这个场景中我们把w,b当做影响射击的因素的估计，反向传播过程也就是不断\n",
    " 的在更新w,b，以达到误差最小）$，观察结果（这时候相当于进行了一次正向传播）。然后发现，偏离靶心左边，应该往右点儿打。所以战士开始\n",
    " 根据偏离靶心的距离（误差，也称损失）调整了射击方向往右一点（这时，完成了一次反向传播）当完成了一次正反向传播，也就完成了一次神经\n",
    " 网络的训练迭代，反复调整射击角度（反复迭代），误差越来越小，战士打得越来越准，神枪手模型也就诞生了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.BP算法推导和计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 参数初始化：\n",
    "    \n",
    "    - 输入：$i_{1} = 0.1,i_{2} = 0.2$\n",
    "    - 输出：$O_{1} = 0.01,O_{2} = 0.99$ 相当于标定了\n",
    "    - 权重：$w_{1} = 0.1,w_{2} = 0.2, w_{3} = 0.3, w_{4} = 0.4\\\\\n",
    "          w_{5} = 0.5,w_{6} = 0.6,w_{7} = 0.7,w_{8} = 0.8$\n",
    "    - 扁置：$b_{1} = 0.55,b_{2} = 0.56,b_{3} = 0.66,b_{4} = 0.67$\n",
    "          \n",
    "- 正向传播：\n",
    "    - 计算隐层神经元$h_{1}$输入加权和：\n",
    "        $$\\begin{aligned}\n",
    "        in_{h_{1}}=&w_{1} \\ast i_{1} + w_{2} \\ast i_{2} + 1 \\ast b_{1}\\space\\space\\space\\space\\space (3.1) \\\\\n",
    "            =& 0.1 \\ast 0.1 + 0.2 \\ast 0.2 + 1 \\ast 0.55 \\\\\n",
    "            =& 0.6\\\\\n",
    "           \\end{aligned}\n",
    "        $$\n",
    "    - 计算隐层神经网络$h_{1}$的输出，需要通过激活函数Sigmoid：\n",
    "        $$\n",
    "        \\begin{aligned}\n",
    "        out_{h_{1}}=& \\frac{1}{e^{-in_{h_{1}}}+1}      \\space\\space\\space\\space\\space (3.2)\\\\\n",
    "                    =&\\frac{1}{e^{0.6}+1} \\\\\n",
    "                    =&0.6456563062\\end{aligned}\n",
    "        $$\n",
    "    - 同理，计算出$h_{2}$的输出：\n",
    "         $$\n",
    "        out_{h_{2}}= 0.6592603884\n",
    "         $$\n",
    "- 隐层-->输出层：\n",
    "    - 计算输出层神经元$O_{1}$的输入加权和：\n",
    "        $$\n",
    "        \\begin{aligned}\n",
    "        in_{O1}=& w_{5} \\ast Out_{h_{1}} + w_{6} \\ast Out_{h_{2}} + 1 \\ast b_{3} \\space\\space\\space\\space\\space (3.3)\\\\\n",
    "                        =&0.5 \\ast 0.6456563062 + 0.6 \\ast 0.6592603884 \\\\\n",
    "                        =& 1.3783843861\\end{aligned}\n",
    "        $$\n",
    "    - 计算隐层$O_{1}$的输出：\n",
    "        $$\n",
    "        \\begin{aligned}\n",
    "        out_{O1}= &\\frac{1}{e^{-in_{O_{1}}}+1}      \\space\\space\\space\\space\\space (3.4)\\\\\n",
    "                        =&\\frac{1}{e^{1.3783843861}+1} \\\\\n",
    "                        =&0.7987314002\n",
    "        \\end{aligned}\n",
    "        $$\n",
    "    - 同理，计算隐层神经元$O_{2}$的输出：\n",
    "        $$\n",
    "        out_{O_{2}}= 0.8374488853\n",
    "        $$\n",
    "        \n",
    "    正向传播结束，我们看看输出层的结果[0.7987314002,0.8374488853],但是我们希望它能输出[0.01,0.99],所以误差相差很大，这个时候我们就需要利用反向传播更新权值w，然后重新计算输出：\n",
    "- 反向传播：\n",
    "    - 1.计算输出误差：\n",
    "    $$\n",
    "       \\begin{aligned}\n",
    "       E_{total} =& \\sum_{i=1}^{\\space2}E_{out_{O_{\\space i}}} \\space\\space\\space\\space\\space (3.5)\\\\\n",
    "       =&E_{out_{O_{\\space 1}}} + E_{out_{O_{\\space2}}}\\\\\n",
    "       =&\\frac{1}{2}\\left(O_{1}-out_{O_{\\space 1}} \\right)^2+\\frac{1}{2}\\left(O_{2}-out_{O_{\\space 2}} \\right)^2\\\\\n",
    "       =&\\frac{1}{2}\\left(0.01-0.7987314002 \\right)^2+\\frac{1}{2}\\left(0.99-0.8374488853 \\right)^2\\\\\n",
    "       =&0.0116359213+0.3110486109\\\\\n",
    "       =&0.3226845322\\\\\n",
    "       \\\\\n",
    "       &\\textrm{其中}{ E_{out_{O_{\\space 1}}}= 0.0116359213 ,E_{out_{O_{\\space 2}}}= 0.3110486109}\n",
    "       \\end{aligned}\n",
    "       $$\n",
    "       \n",
    "         注意：这里用平方和损失作为误差的计算，是因为它简单，实际上用的时候效果不咋滴，如果激活函数是饱和的，带来的缺陷就是系统迭代更新更慢，系统收敛就慢，当然这可以通过交叉熵函数作为损失函数来弥补，交叉熵作为代价函数在计算误差对输入的梯度时，抵消了激活函数的导数项，神奇的避免了因为激活函数的“饱和性”给系统带来的负面影响. [传送门](05-BP神经网络-二次代价到交叉熵代价.ipynb)\n",
    "\n",
    "            交叉熵：\n",
    "\n",
    "        $$\n",
    "        \\begin{aligned}\n",
    "         E_{total} = &-\\frac{1}{m}\\sum_{i=1}^{m}\\left(O\\cdot log(out_{O})+(1-O)\\cdot log(1- out_{O})\\right)\\\\\n",
    "        \\frac{\\partial out_{O}}{\\partial out_{in_{O}}} =& out_{O}(1-out_{O})\n",
    "        \\end{aligned}$$\n",
    "\n",
    "            对输出的偏导数：\n",
    "         $$\n",
    "            \\begin{aligned}\n",
    "                \\frac{\\partial E_{total}}{\\partial out_{O}} = &-\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\frac{O}{out_{O}} - \\frac{1-O}{1- Out_{O}}\\right)\\frac{\\partial out_{O}}{\\partial out_{in_{O}}}\\\\\n",
    "            =&-\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\frac{O}{out_{O}} - \\frac{1-O}{1-Out_{O}}\\right)out_{O}(1-out_{O}) \\\\\n",
    "            =& -\\frac{1}{m}\\sum_{i=1}^{m}\\left(O-out_{O}\\right)\n",
    "            \\end{aligned}\n",
    "         $$\n",
    "      \n",
    "    - 2.隐层-->输出层的权值及偏置b的更新：\n",
    "      \n",
    "    - 先给出链式求导法则：\n",
    "              \n",
    "               假设y是u的函数，而u是x的函数：y = f(u),u = g(x)\n",
    "               \n",
    "               对应的复合函数就是：y = f(g(x))\n",
    "               \n",
    "               那么y对x的导数则有： \n",
    "         $$            \n",
    "               \\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n",
    "         $$ \n",
    "    - 以更新w5为例：\n",
    "            as we know ,全值w的大小能直接影响输出，w不合适那么会使得输出误差。要想知道某一个w值对误差影响的程度，可以用误差对该w的变化率来表达，如果w的一点点变动，就会导致误差增大很多，说明这个w对误差影响的程度就更大，也就是说，误差对该w的变化率越高。而误差对w的变化率就是误差对w的偏导。看下图，总误差的大小首先受输出神经元O1的输出影响，继续反推，O1的输出受它自己的输入影响，而输入又收到w5的影响。这就是链式反应，从结果反推根因。\n",
    " ![04-05.jpg](./data/04-05.jpg)\n",
    "       那么根据链式法则则有：\n",
    "         $$\n",
    "                \\frac{\\partial E_{total}}{\\partial W_{5}} = \\frac{\\partial E_{total}}{\\partial out_{O1}}\\cdot \\frac{\\partial out_{O1}}{\\partial in_{O1}}\\cdot \\frac{\\partial in_{O1}}{\\partial W_{5}}           \\space\\space\\space\\space (3.6)\n",
    "         $$\n",
    "\n",
    "\n",
    "     现在开始逐个计算：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\because  E_{total} =& \\sum_{i=1}^{\\space2}E_{out_{O_{\\space i}}} \\space\\space\\space\\space\\space (3.7)\\\\\n",
    "=&E_{out_{O_{\\space 1}}} + E_{out_{O_{\\space2}}}\\\\\n",
    "=&\\frac{1}{2}\\left(O_{1}-out_{O_{\\space 1}} \\right)^2+\\frac{1}{2}\\left(O_{2}-out_{O_{\\space 2}} \\right)^2\\\\\n",
    "\\therefore\\frac{\\partial E_{total}}{\\partial out_{O1}} = &\\frac{\\frac{1}{2}\\left(O_{1}-out_{O_{\\space 1}} \\right)^2+\\frac{1}{2}\\left(O_{2}-out_{O_{\\space 2}} \\right)^2}{\\partial out_{O1}}  \\space\\space\\space\\space (2.7) \\\\\n",
    "=&2\\cdot\\frac{1}{2}(O_{1} - out_{O1})^{2-1}\\cdot (0-1) +0\\\\\n",
    "=&-(O_{1} - out_{O1}) \\space\\space\\space\\space (3.8)\\\\\n",
    "=&-(0.01-0.7987314002)\\\\\n",
    "=&0.7887314002\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    " $$\n",
    "        \\begin{aligned}\n",
    "        \\because out_{O1}= &\\frac{1}{e^{-in_{O_{1}}}+1}      \\space\\space\\space\\space\\space (3.9)\\\\\n",
    "        \\therefore \\frac{\\partial out_{O1}}{\\partial in_{O1}}= &\\frac{\\partial \\frac{1}{e^{\\space -in_{\\space O_{1}}\\space}\\space+\\space1}}{\\partial in_{O1}}  \\\\  \n",
    "        =&out_{O1}(1-out_{O1}) \\space\\space\\space\\space\\space (3.10)\\\\\n",
    "        =&0.7987314002 \\cdot(1-0.7987314002)\\\\\n",
    "        =&0.1607595505\n",
    "        \\end{aligned}\n",
    "$$\n",
    "\n",
    " $$\n",
    "        \\begin{aligned}\n",
    "        \\because   in_{O1}=& w_{5} \\ast Out_{h_{1}} + w_{6} \\ast Out_{h_{2}} + 1 \\ast b_{3} \\space\\space\\space\\space\\space (3.11)\\\\\n",
    "        \\therefore  \\frac{\\partial in_{O1}}{\\partial W_{5}} \\\\= &\\frac{\\partial {w_{5} \\ast Out_{h_{1}} + w_{6} \\ast Out_{h_{2}} + 1 \\ast b_{3}}}{\\partial w_{5}}  \\space\\space\\space\\space\\space (3.12)\\\\\n",
    "         =&1\\cdot w_{5}^{1-1} \\cdot out_{h1} + 0 + 0\\\\\n",
    "         =&out_{h1}\\\\\n",
    "         =&0.6456563062\n",
    "        \\end{aligned}\n",
    "$$\n",
    "\n",
    " $$\n",
    "        \\begin{aligned}\n",
    "        \\therefore \\frac{\\partial E_{total}}{\\partial W_{5}} =& \\frac{\\partial E_{total}}{\\partial out_{O1}}\\cdot \\frac{\\partial out_{O1}}{\\partial in_{O1}}\\cdot \\frac{\\partial in_{O1}}{\\partial W_{5}}  \\space\\space\\space\\space\\space (3.13)\\\\\n",
    "        =&0.7887314002\\cdot 0.1607595505 \\cdot 0.6456563062\\\\\n",
    "        =&0.0818667051\n",
    "        \\end{aligned}\n",
    "$$\n",
    "\n",
    "    对w求偏导公式可以归纳为：\n",
    " \n",
    "$$\n",
    "        \\begin{aligned}\n",
    "        \\therefore \\frac{\\partial E_{total}}{\\partial W_{5}} =& \\frac{\\partial E_{total}}{\\partial out_{O1}}\\cdot \\frac{\\partial out_{O1}}{\\partial in_{O1}}\\cdot \\frac{\\partial in_{O1}}{\\partial W_{5}}  \\space\\space\\space\\space\\space (3.13)\\\\\n",
    "        =&(-(O_{1} - out_{O1})) \\cdot out_{O1}\\cdot(1-out_{O1}) \\cdot out_{h1}\\\\\n",
    "        =&\\sigma_{O_{1}}\\cdot out_{h1}\\\\\n",
    "        \\textrm{其中,}\\sigma_{O_{1}}=& (-(O_{1} - out_{O1})) \\cdot out_{O1}\\cdot(1-out_{O1}) \\cdot out_{h1}\n",
    "        \\end{aligned}\n",
    "$$\n",
    "\n",
    "   同理，对b求偏导公式可以归纳为：\n",
    "$$\n",
    "        \\begin{aligned}\n",
    "        \\because   in_{O1}=& w_{5} \\ast Out_{h_{1}} + w_{6} \\ast Out_{h_{2}} + 1 \\ast b_{3} \\space\\space\\space\\space\\space (3.14)\\\\\n",
    "        \\therefore \\frac{\\partial in_{O1}}{\\partial  b_{3}}=&\\frac{\\partial {w_{5} \\ast Out_{h_{1}} + w_{6} \\ast Out_{h_{2}} + 1 \\ast b_{3}}}{\\partial  b_{3}}  \\space\\space\\space\\space\\space (3.15)\\\\\n",
    "        =&0+0+b_{3}^{1-1}\\\\\n",
    "        =&1\n",
    "        \\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "        \\begin{aligned}\n",
    "        \\therefore \\frac{\\partial E_{total}}{\\partial  b_{3}} =& \\frac{\\partial E_{total}}{\\partial out_{O1}}\\cdot \\frac{\\partial out_{O1}}{\\partial in_{O1}}\\cdot \\frac{\\partial in_{O1}}{\\partial  b_{3}}  \\space\\space\\space\\space\\space (3.16)\\\\\n",
    "        =&(-(O_{1} - out_{O1})) \\cdot out_{O1}\\cdot(1-out_{O1}) \\cdot 1\\\\\n",
    "        =&\\sigma_{O_{1}}\\\\\n",
    "        \\textrm{其中,}\\sigma_{O_{1}}=& (-(O_{1} - out_{O1})) \\cdot out_{O1}\\cdot(1-out_{O1}) \n",
    "        \\end{aligned}\n",
    "$$\n",
    "\n",
    "更新w5值之前还需要初始化一个学习率，关于学习率，不能过高也不能过低，因为训练神经网络系统的过程，就是通过不断的迭代，找到让系统\n",
    "输出误差最小的参数的过程。每一次迭代都经过反向传播进行梯度下降。学习率太小，容易陷入局部最优。学习率太高，系统可能无法收敛，\n",
    "会在一个地方上窜下跳。\n",
    "\n",
    "顺带提一句，神经网络的目标是通过更新w，b的值来实现误差最小，因此顺着w,b的梯度的反方向，下降的最快。\n",
    "\n",
    "现在开始更新w5的值，就设定学习率为0.5吧：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w_{5}^{+} =& w_{5} - \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial w_{5}} \\space\\space\\space\\space (3.17)\\\\\n",
    "=&0.5 - 0.5 \\cdot 0.0818667051 \\\\\n",
    "=&0.45906664745\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "归纳输出层w更新的公式：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w_{O}^{+} =&w_{O} - \\alpha \\cdot(-(O - out_{O})) \\cdot out_{O}\\cdot(1-out_{O}) \\cdot out_{h} \\space\\space\\space\\space (3.18)\\\\\n",
    "=&w_{O} + \\alpha \\cdot(O - out_{O})\\cdot out_{O}\\cdot(1-out_{O}) \\cdot out_{h}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "同理可以计算$w_{6},w_{7},w_{8}$的更新值。\n",
    "        \n",
    "同理更新偏置b：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "b^{+} =& b _{O}- \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial b_{O}} \\space\\space\\space\\space (3.19)\\\\\n",
    "=&b_{O}- \\alpha \\cdot(-(O_{1} - out_{O1})) \\cdot out_{O1}\\cdot(1-out_{O1}) \\\\\n",
    "=&b_{O}+ \\alpha \\cdot(O_{1} - out_{O1}) \\cdot out_{O1}\\cdot(1-out_{O1})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 3.输入层-->隐层的全值及偏置b更新：\n",
    "    - 3.1以更新w1为例：\n",
    "        ![04-06.jpg](./data/04-06.jpg)\n",
    "        从上图中可以看出，我们在求$w_{5}$的更新时，误差反向传递路径为：输出层->隐层，即 $out_{O1}-->in_{O1}-->w_{5}$,总误差只有一条路径能传回来。但是在求$w_{1}$时，误差反向传递路径是：隐藏层-->输入层，但是隐藏层的神经元是有2根线的，所以总误差沿着2个路径回来，也就是说，计算偏导时，要分开计算。\n",
    "     总误差对$w_{1}$的偏导：\n",
    "     $$\n",
    "     \\begin{aligned}\n",
    "     \\frac{\\partial E_total}{w_{1}} = &\\frac{\\partial E_total}{\\partial Out_{h1}}\\cdot\\frac{\\partial out_{h1}}{\\partial in_{h1}}\\cdot\\frac{\\partial in_{h1}}{w_{1}} \\space\\space\\space\\space (3.20) \\\\\n",
    "     =&(\\frac{\\partial E_{O1}}{\\partial Out_{h1}}+\\frac{\\partial E_{O2}}{\\partial Out_{h1}})\\cdot\\frac{\\partial out_{h1}}{\\partial in_{h1}}\\cdot\\frac{\\partial in_{h1}}{w_{1}}\\space\\space\\space\\space (3.21) \\\\\n",
    "     \\end{aligned}\n",
    "     $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 3.2先算：\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial E_total}{\\partial Out_{h1}} = \\frac{\\partial E_{O1}}{\\partial Out_{h1}}+\\frac{\\partial E_{O2}}{\\partial Out_{h1}}\\space\\space\\space\\space (3.22) \\\\\n",
    "\\end{aligned}\\\\\n",
    "$$\n",
    "    \n",
    "    挨个算： \n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial E_{O1}}{\\partial Out_{h1}} = \\frac{\\partial E_{O1}}{\\partial in_{O1}}\\cdot\\frac{\\partial in_{O1}}{\\partial out_{h1}}\\space\\space\\space\\space (3.23) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "      计算左边部分, 参考3.6,3.7,3.8：\n",
    "$$\\begin{aligned}\n",
    "        \\frac{\\partial E_{O1}}{\\partial in_{O1}} = &\\frac{\\partial E_{O1}}{\\partial out_{O1}}\\cdot \\frac{\\partial out_{O1}}{\\partial in_{O1}}           \\space\\space\\space\\space (3.24)\\\\\n",
    "        =&\\frac{\\partial\\frac{1}{2}\\left(O_{1}-out_{O_{\\space 1}} \\right)^2}{\\partial out_{O1}}\\cdot\\frac{\\partial out_{O1}}{\\partial in_{O1}}\\\\\n",
    "        =&-(O_{1} - out_{O1}) \\cdot \\frac{\\partial out_{O1}}{\\partial in_{O1}}\\\\\n",
    "        =&0.7987314002 \\cdot 0.16075955505\\\\\n",
    "        =&0.1284037009\n",
    " \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 计算右边部分：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\because   in_{O1}=& w_{5} \\ast Out_{h_{1}} + w_{6} \\ast Out_{h_{2}} + 1 \\ast b_{3} \\space\\space\\space\\space\\space (3.25)\\\\\n",
    "\\therefore \\frac{\\partial in_{O1}}{\\partial  out_{h1}}=&\\frac{\\partial {w_{5} \\ast Out_{h_{1}} + w_{6} \\ast Out_{h_{2}} + 1 \\ast b_{3}}}{\\partial out_{h1}}  \\space\\space\\space\\space\\space (3.26)\\\\\n",
    "=&w_{5} \\cdot out_{h1}^{1-1} + 0 + 0\\\\\n",
    "=&w_{5}   \\space\\space\\space\\space\\space (3.27)\\\\\n",
    "=&0.5\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    " $$\\begin{aligned}\n",
    "\\therefore \\frac{\\partial E_{O1}}{\\partial Out_{h1}} = &\\frac{\\partial E_{O1}}{\\partial in_{O1}}\\cdot\\frac{\\partial in_{O1}}{\\partial out_{h1}}\\space\\space\\space\\space (3.28) \\\\\n",
    "=&0.0.1284037009 \\cdot 0.5 \\\\\n",
    "=&0.06420185045\n",
    "\\end{aligned}\n",
    " $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    同理：\n",
    "  $$\\begin{aligned}\n",
    "        \\therefore \\frac{\\partial E_{O2}}{\\partial Out_{h1}} = &\\frac{\\partial E_{O2}}{\\partial in_{O2}}\\cdot\\frac{\\partial in_{O2}}{\\partial out_{h1}}\\space\\space\\space\\space (3.29) \\\\\n",
    "        =&-(O_{2} - out_{O2}) \\cdot \\frac{\\partial out_{O2}}{\\partial in_{O2}}\\cdot \\frac{in_{O2}}{out_{h1}}\\\\\n",
    "        =&-(O_{2} - out_{O2}) \\cdot out_{O2}(1-out_{O2})\\cdot w_{7}\\\\\n",
    "        =&-(0.99-0.8374488853)\\cdot 0.8374488853 \\cdot (1-0.8374488853)\\cdot 0.7\\\\\n",
    "        =&-0.0145365614\n",
    "  \\end{aligned}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     所以：\n",
    " $$\\begin{aligned}\n",
    "        \\frac{\\partial E_total}{\\partial Out_{h1}} = &\\frac{\\partial E_{O1}}{\\partial Out_{h1}}+\\frac{\\partial E_{O2}}{\\partial Out_{h1}}\\space\\space\\space\\space (3.30) \\\\\n",
    "        =&0.06420185045+(-0.0145365614)\\\\\n",
    "        =&0.04966528905\n",
    "        \\end{aligned}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 3.2 然后算：\n",
    "  $\n",
    "  \\frac{\\partial out_{h1}}{\\partial in_{h1}}\n",
    "  $\n",
    "  $$\n",
    "        \\begin{aligned}\n",
    "        \\because out_{h1}= &\\frac{1}{e^{-in_{h_{1}}}+1}      \\space\\space\\space\\space\\space (3.31)\\\\\n",
    "        \\therefore \\frac{\\partial out_{h1}}{\\partial in_{h1}}= &\\frac{\\partial \\frac{1}{e^{\\space -in_{\\space O_{1}}\\space}\\space+\\space1}}{\\partial in_{h1}}  \\\\  \n",
    "        =&out_{h1}(1-out_{h1}) \\space\\space\\space\\space\\space (3.32)\\\\\n",
    "        =&0.06456563062 \\cdot(1-0.06456563062)\\\\\n",
    "        =&0.2287842405\n",
    "        \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.3 最后算： \n",
    " $$\n",
    " \\begin{aligned}\\frac{\\partial in_{h1}}{w_{1}}=&\\frac{\\partial(w_{1} \\cdot i_{1} + w_{2} \\cdot i_{2} )}{\\partial w_{1}}\\\\\n",
    " =&w_{1}^{1-1} \\cdot i_{1} + 0 +0 \\\\\n",
    " =&i_{1}\\\\\n",
    " =&0.1\n",
    "  \\end{aligned}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 3.4 最后，将3者相乘：\n",
    "  $$\n",
    " \\begin{aligned}\n",
    " \\frac{\\partial E_total}{w_{1}} = &\\frac{\\partial E_total}{\\partial Out_{h1}}\\cdot\\frac{\\partial out_{h1}}{\\partial in_{h1}}\\cdot\\frac{\\partial in_{h1}}{w_{1}} \\space\\space\\space\\space (3.20) \\\\\n",
    " =&(\\frac{\\partial E_{O1}}{\\partial Out_{h1}}+\\frac{\\partial E_{O2}}{\\partial Out_{h1}})\\cdot\\frac{\\partial out_{h1}}{\\partial in_{h1}}\\cdot\\frac{\\partial in_{h1}}{w_{1}}\\space\\space\\space\\space (3.21) \\\\\n",
    " =&0.04966528905 \\cdot 0.2287842405 \\cdot 0.1\\\\\n",
    " =&0.0011362635\n",
    " \\end{aligned}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们归纳一下式子：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial E_total}{w_{1}} = &\\frac{\\partial E_total}{\\partial Out_{h1}}\\cdot\\frac{\\partial out_{h1}}{\\partial in_{h1}}\\cdot\\frac{\\partial in_{h1}}{w_{1}}   \\\\\n",
    "=&(\\frac{\\partial E_{O1}}{\\partial Out_{h1}}+\\frac{\\partial E_{O2}}{\\partial Out_{h1}})\\cdot\\frac{\\partial out_{h1}}{\\partial in_{h1}}\\cdot\\frac{\\partial in_{h1}}{w_{1}}\\\\\n",
    "=&(\\sum_{O}^{}\\frac{\\partial E_{O}}{\\partial out_{O}}\\cdot\\frac{\\partial out_{O}}{\\partial in_{O}}\\cdot\\frac{\\partial in_{O}}{\\partial out_{h}})\\cdot\\frac{\\partial out_{h1}}{\\partial in_{h1}}\\cdot\\frac{\\partial in_{h1}}{w_{1}}\\space\\space\\space\\space\\space (3.33)\\\\\\\\ \n",
    "=&(\\sum_{O}\\sigma_{O}W_{O})\\cdot out_{h1}(1-out_{h1})\\cdot i_{1} \\space\\space\\space\\space\\space (3.33)\\\\\n",
    "=&\\sigma_{h1} \\cdot i_{1}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$\\sigma_{h1} = (\\sum_{O}\\sigma_{O}W_{O})\\cdot out_{h1}(1-out_{h1})$\n",
    "\n",
    "$\\sigma_{O}$看做输出层的误差量，然后改误差量和$W$相乘，相当于通过w传播了过来，如果是深层网络，\n",
    "隐藏层数量>1，那么公式中的$\\sigma_{O}$写成$\\sigma_{h}$,$W_{O}$写成$W_{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " 终于，可以更新$W1$的值了：\n",
    "    $$\\begin{aligned}\n",
    "    W_{1}^{+} = &W_{1} - \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial W_{1}}\\\\\n",
    "    =&0.1-0.1 \\cdot 0.0011362635\\\\\n",
    "    =&0.0998863737\n",
    "    \\end{aligned}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归纳一下，隐藏层$W$的更新的公式：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "W_{h}^{+} =&W_{h} - \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial W}\\\\\n",
    "=&W_{h} + \\alpha \\cdot (-\\sum_{O}\\sigma_{O}W_{O})\\cdot out_{h1}(1-out_{h})\\cdot i   \\space\\space\\space\\space (3.34)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果隐藏层数量>1:\n",
    "$$\\begin{aligned}\n",
    "W_{h}^{+} =&W_{h} - \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial W}\\\\\n",
    "=&W_{h} + \\alpha \\cdot (-\\sum_{hh}\\sigma_{hh}W_{hh})\\cdot out_{h1}(1-out_{h})\\cdot in_{h}    \\space\\space\\space\\space (3.35)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "    hh代表当前隐藏层的下一个隐藏层（-->正向），深层网络，计算的式子就是递归计算，同理，可以计算出W2,W3,W4的更新值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理隐藏层偏置b的更新：\n",
    "$$\\begin{aligned}\n",
    " \\frac{\\partial E_{total}}{\\partial b_{h}}=&(\\sum_{h}\\sigma_{h}W_{h})\\cdot out_{h}(1-out_{h})  \\space\\space\\space\\space (3.36)\n",
    "\\end{aligned}\n",
    "$$   \n",
    "    \n",
    "$$\\begin{aligned}\n",
    "b_{h}^{+} =&b_{h} - \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial b_{h}}\\\\\n",
    "=&b_{h} + \\alpha \\cdot (-\\sum_{h}\\sigma_{h}W_{h})\\cdot out_{h}(1-out_{h})  \\space\\space\\space\\space (3.37)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "如果隐藏层数量>1:\n",
    "$$\\begin{aligned}\n",
    "b_{h}^{+} =&b_{h} - \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial b_{h}}\\\\\n",
    "=&b_{h} + \\alpha \\cdot (-\\sum_{hh}\\sigma_{hh}W_{hh})\\cdot out_{h}(1-out_{h})  \\space\\space\\space\\space (3.38)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "hh代表当前隐藏层的下一个隐藏层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四.代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看懂算法之后，通过手动实现一遍实现一遍神经网络，可以加深对算法原理的理解，以及对算法实现思路的理解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.shape(xy)\n",
      "(2, 60)\n",
      "colors:\n",
      " [1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
      " 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1]\n",
      "(60, 2)\n",
      "(2, 60)\n",
      "expect_output [[0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1\n",
      "  1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0]\n",
      " [1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0\n",
      "  0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1]]\n",
      "self.x:\n",
      " [[-0.01570789  2.13305022  0.49367916  0.45469055  0.71123229  1.65093347\n",
      "   2.05232247  0.33626079  0.52157802  0.58195238 -0.74877056  2.75572984\n",
      "   1.86033616 -0.32057618  0.97617873  0.58451774 -1.96506379 -0.21613181\n",
      "   2.22272185 -0.46959835  0.54893466  1.79194037  0.49242714  1.01975447\n",
      "   1.72611511  1.60152079 -0.46156048  0.54569332  1.85825492 -0.61227019\n",
      "  -2.2730765   0.17909405  0.02683149 -0.27689167 -0.77154764  0.60466012\n",
      "  -0.87661629 -0.85347741 -2.82996377  1.26441198  2.772976    1.17277116\n",
      "   0.22230766 -1.56625249 -0.37131705  0.89027345  0.65842616 -0.6303501\n",
      "   1.48270988  0.11946719  0.10805509  0.66266304  0.52026131  2.58062729\n",
      "  -1.1310448   0.48465556  1.01478664 -0.4679288   2.01193036 -1.23160509]\n",
      " [-0.41302706  0.7316644  -1.59165857 -0.4987107  -0.27706577 -2.17847537\n",
      "   1.33766174 -1.56856127  0.26565349  0.45332225  1.53081678 -0.23247736\n",
      "  -2.71232161  1.71148354  1.3103928  -0.85294057  2.09507213  2.13949437\n",
      "   0.14157634  0.57621942 -1.01652514  0.75309074  1.69923199 -0.3465502\n",
      "  -1.52096377  0.10991899 -0.70762447  0.98563432 -0.69023702  1.59684553\n",
      "   0.48293173 -0.22060928 -0.141126   -1.08718362 -0.37720204 -0.15052973\n",
      "   0.22395256 -0.85413244 -1.03198595  0.86644942  1.89091337  0.48124503\n",
      "  -1.29183523  0.42731266  1.71967382  0.32200493 -1.21096538 -0.15001624\n",
      "   0.14358312  1.15499481  1.39574185  0.11074674  1.68516365 -1.90641729\n",
      "   0.97537559  1.66978088  0.25086811  0.70269788 -0.22217436  0.87012559]]\n",
      "self.x:\n",
      " [[-0.01570789  2.13305022  0.49367916  0.45469055  0.71123229  1.65093347\n",
      "   2.05232247  0.33626079  0.52157802  0.58195238 -0.74877056  2.75572984\n",
      "   1.86033616 -0.32057618  0.97617873  0.58451774 -1.96506379 -0.21613181\n",
      "   2.22272185 -0.46959835  0.54893466  1.79194037  0.49242714  1.01975447\n",
      "   1.72611511  1.60152079 -0.46156048  0.54569332  1.85825492 -0.61227019\n",
      "  -2.2730765   0.17909405  0.02683149 -0.27689167 -0.77154764  0.60466012\n",
      "  -0.87661629 -0.85347741 -2.82996377  1.26441198  2.772976    1.17277116\n",
      "   0.22230766 -1.56625249 -0.37131705  0.89027345  0.65842616 -0.6303501\n",
      "   1.48270988  0.11946719  0.10805509  0.66266304  0.52026131  2.58062729\n",
      "  -1.1310448   0.48465556  1.01478664 -0.4679288   2.01193036 -1.23160509]\n",
      " [-0.41302706  0.7316644  -1.59165857 -0.4987107  -0.27706577 -2.17847537\n",
      "   1.33766174 -1.56856127  0.26565349  0.45332225  1.53081678 -0.23247736\n",
      "  -2.71232161  1.71148354  1.3103928  -0.85294057  2.09507213  2.13949437\n",
      "   0.14157634  0.57621942 -1.01652514  0.75309074  1.69923199 -0.3465502\n",
      "  -1.52096377  0.10991899 -0.70762447  0.98563432 -0.69023702  1.59684553\n",
      "   0.48293173 -0.22060928 -0.141126   -1.08718362 -0.37720204 -0.15052973\n",
      "   0.22395256 -0.85413244 -1.03198595  0.86644942  1.89091337  0.48124503\n",
      "  -1.29183523  0.42731266  1.71967382  0.32200493 -1.21096538 -0.15001624\n",
      "   0.14358312  1.15499481  1.39574185  0.11074674  1.68516365 -1.90641729\n",
      "   0.97537559  1.66978088  0.25086811  0.70269788 -0.22217436  0.87012559]]\n",
      "self.x:\n",
      " [[-0.01570789  2.13305022  0.49367916  0.45469055  0.71123229  1.65093347\n",
      "   2.05232247  0.33626079  0.52157802  0.58195238 -0.74877056  2.75572984\n",
      "   1.86033616 -0.32057618  0.97617873  0.58451774 -1.96506379 -0.21613181\n",
      "   2.22272185 -0.46959835  0.54893466  1.79194037  0.49242714  1.01975447\n",
      "   1.72611511  1.60152079 -0.46156048  0.54569332  1.85825492 -0.61227019\n",
      "  -2.2730765   0.17909405  0.02683149 -0.27689167 -0.77154764  0.60466012\n",
      "  -0.87661629 -0.85347741 -2.82996377  1.26441198  2.772976    1.17277116\n",
      "   0.22230766 -1.56625249 -0.37131705  0.89027345  0.65842616 -0.6303501\n",
      "   1.48270988  0.11946719  0.10805509  0.66266304  0.52026131  2.58062729\n",
      "  -1.1310448   0.48465556  1.01478664 -0.4679288   2.01193036 -1.23160509]\n",
      " [-0.41302706  0.7316644  -1.59165857 -0.4987107  -0.27706577 -2.17847537\n",
      "   1.33766174 -1.56856127  0.26565349  0.45332225  1.53081678 -0.23247736\n",
      "  -2.71232161  1.71148354  1.3103928  -0.85294057  2.09507213  2.13949437\n",
      "   0.14157634  0.57621942 -1.01652514  0.75309074  1.69923199 -0.3465502\n",
      "  -1.52096377  0.10991899 -0.70762447  0.98563432 -0.69023702  1.59684553\n",
      "   0.48293173 -0.22060928 -0.141126   -1.08718362 -0.37720204 -0.15052973\n",
      "   0.22395256 -0.85413244 -1.03198595  0.86644942  1.89091337  0.48124503\n",
      "  -1.29183523  0.42731266  1.71967382  0.32200493 -1.21096538 -0.15001624\n",
      "   0.14358312  1.15499481  1.39574185  0.11074674  1.68516365 -1.90641729\n",
      "   0.97537559  1.66978088  0.25086811  0.70269788 -0.22217436  0.87012559]]\n",
      "self.x:\n",
      " [[-0.01570789  2.13305022  0.49367916  0.45469055  0.71123229  1.65093347\n",
      "   2.05232247  0.33626079  0.52157802  0.58195238 -0.74877056  2.75572984\n",
      "   1.86033616 -0.32057618  0.97617873  0.58451774 -1.96506379 -0.21613181\n",
      "   2.22272185 -0.46959835  0.54893466  1.79194037  0.49242714  1.01975447\n",
      "   1.72611511  1.60152079 -0.46156048  0.54569332  1.85825492 -0.61227019\n",
      "  -2.2730765   0.17909405  0.02683149 -0.27689167 -0.77154764  0.60466012\n",
      "  -0.87661629 -0.85347741 -2.82996377  1.26441198  2.772976    1.17277116\n",
      "   0.22230766 -1.56625249 -0.37131705  0.89027345  0.65842616 -0.6303501\n",
      "   1.48270988  0.11946719  0.10805509  0.66266304  0.52026131  2.58062729\n",
      "  -1.1310448   0.48465556  1.01478664 -0.4679288   2.01193036 -1.23160509]\n",
      " [-0.41302706  0.7316644  -1.59165857 -0.4987107  -0.27706577 -2.17847537\n",
      "   1.33766174 -1.56856127  0.26565349  0.45332225  1.53081678 -0.23247736\n",
      "  -2.71232161  1.71148354  1.3103928  -0.85294057  2.09507213  2.13949437\n",
      "   0.14157634  0.57621942 -1.01652514  0.75309074  1.69923199 -0.3465502\n",
      "  -1.52096377  0.10991899 -0.70762447  0.98563432 -0.69023702  1.59684553\n",
      "   0.48293173 -0.22060928 -0.141126   -1.08718362 -0.37720204 -0.15052973\n",
      "   0.22395256 -0.85413244 -1.03198595  0.86644942  1.89091337  0.48124503\n",
      "  -1.29183523  0.42731266  1.71967382  0.32200493 -1.21096538 -0.15001624\n",
      "   0.14358312  1.15499481  1.39574185  0.11074674  1.68516365 -1.90641729\n",
      "   0.97537559  1.66978088  0.25086811  0.70269788 -0.22217436  0.87012559]]\n",
      "self.x:\n",
      " [[-0.01570789  2.13305022  0.49367916  0.45469055  0.71123229  1.65093347\n",
      "   2.05232247  0.33626079  0.52157802  0.58195238 -0.74877056  2.75572984\n",
      "   1.86033616 -0.32057618  0.97617873  0.58451774 -1.96506379 -0.21613181\n",
      "   2.22272185 -0.46959835  0.54893466  1.79194037  0.49242714  1.01975447\n",
      "   1.72611511  1.60152079 -0.46156048  0.54569332  1.85825492 -0.61227019\n",
      "  -2.2730765   0.17909405  0.02683149 -0.27689167 -0.77154764  0.60466012\n",
      "  -0.87661629 -0.85347741 -2.82996377  1.26441198  2.772976    1.17277116\n",
      "   0.22230766 -1.56625249 -0.37131705  0.89027345  0.65842616 -0.6303501\n",
      "   1.48270988  0.11946719  0.10805509  0.66266304  0.52026131  2.58062729\n",
      "  -1.1310448   0.48465556  1.01478664 -0.4679288   2.01193036 -1.23160509]\n",
      " [-0.41302706  0.7316644  -1.59165857 -0.4987107  -0.27706577 -2.17847537\n",
      "   1.33766174 -1.56856127  0.26565349  0.45332225  1.53081678 -0.23247736\n",
      "  -2.71232161  1.71148354  1.3103928  -0.85294057  2.09507213  2.13949437\n",
      "   0.14157634  0.57621942 -1.01652514  0.75309074  1.69923199 -0.3465502\n",
      "  -1.52096377  0.10991899 -0.70762447  0.98563432 -0.69023702  1.59684553\n",
      "   0.48293173 -0.22060928 -0.141126   -1.08718362 -0.37720204 -0.15052973\n",
      "   0.22395256 -0.85413244 -1.03198595  0.86644942  1.89091337  0.48124503\n",
      "  -1.29183523  0.42731266  1.71967382  0.32200493 -1.21096538 -0.15001624\n",
      "   0.14358312  1.15499481  1.39574185  0.11074674  1.68516365 -1.90641729\n",
      "   0.97537559  1.66978088  0.25086811  0.70269788 -0.22217436  0.87012559]]\n",
      "self.x:\n",
      " [[-0.01570789  2.13305022  0.49367916  0.45469055  0.71123229  1.65093347\n",
      "   2.05232247  0.33626079  0.52157802  0.58195238 -0.74877056  2.75572984\n",
      "   1.86033616 -0.32057618  0.97617873  0.58451774 -1.96506379 -0.21613181\n",
      "   2.22272185 -0.46959835  0.54893466  1.79194037  0.49242714  1.01975447\n",
      "   1.72611511  1.60152079 -0.46156048  0.54569332  1.85825492 -0.61227019\n",
      "  -2.2730765   0.17909405  0.02683149 -0.27689167 -0.77154764  0.60466012\n",
      "  -0.87661629 -0.85347741 -2.82996377  1.26441198  2.772976    1.17277116\n",
      "   0.22230766 -1.56625249 -0.37131705  0.89027345  0.65842616 -0.6303501\n",
      "   1.48270988  0.11946719  0.10805509  0.66266304  0.52026131  2.58062729\n",
      "  -1.1310448   0.48465556  1.01478664 -0.4679288   2.01193036 -1.23160509]\n",
      " [-0.41302706  0.7316644  -1.59165857 -0.4987107  -0.27706577 -2.17847537\n",
      "   1.33766174 -1.56856127  0.26565349  0.45332225  1.53081678 -0.23247736\n",
      "  -2.71232161  1.71148354  1.3103928  -0.85294057  2.09507213  2.13949437\n",
      "   0.14157634  0.57621942 -1.01652514  0.75309074  1.69923199 -0.3465502\n",
      "  -1.52096377  0.10991899 -0.70762447  0.98563432 -0.69023702  1.59684553\n",
      "   0.48293173 -0.22060928 -0.141126   -1.08718362 -0.37720204 -0.15052973\n",
      "   0.22395256 -0.85413244 -1.03198595  0.86644942  1.89091337  0.48124503\n",
      "  -1.29183523  0.42731266  1.71967382  0.32200493 -1.21096538 -0.15001624\n",
      "   0.14358312  1.15499481  1.39574185  0.11074674  1.68516365 -1.90641729\n",
      "   0.97537559  1.66978088  0.25086811  0.70269788 -0.22217436  0.87012559]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6oAAAQpCAYAAADI//DmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3X+0ZXddH/z3h4REiAhqBqX5QeJjENNUjIwQhQo8xMck\nSlJZ/khaDFBqWjXaPlJoKC7AUFsfWFbb1SBExQBWQkSFKY2NFYKKJZgJPyJJGhhDINNkkQBJSkFM\nQj7PH3sPc7m5k3sy996Z79x5vdba6569z/fs8z175s5n3t/93ftUdwcAAABG8bD93QEAAABYSlAF\nAABgKIIqAAAAQxFUAQAAGIqgCgAAwFAEVQAAAIZy6P7uAOwvVfWYJB9I8vvd/a+XbP+mJP8gyfck\n+eEkr0/yqCQvSHJJko8k+cPuvv0hvNfPJPnFJM/o7uuq6pFJrk1yTXf/+JJ2xyW5KskLu/uPHmR/\nfyfJc/PA3+H7k7y7u69b0vaVSZ7U3c+d19+c5K+6+7WL9h8A9gW1WW2Gr+hui+WgWDIVuC8l6SS3\nJ7lyfnxnkn+T5B/P7V48b1+6fDzJ7yV5a5LPJXnJkrZfWqF9ZypMv5LkB5N8et52c5IfTfJr8/pn\nkrwiyZPn/b1+3n5+kncu298XkzxnbvcLe3jPTnLp3Obbk9yS5NwkP5fk+iTPn9v86bJj86Z5+/VJ\nPjovH5u3/daCx/e9c/vj9veftcVisVgOjEVt3rjanOR759d+Nskr9veftcXyUBdnVDmYPCnJIdld\nqJ6c6R/vSvLyJPdU1e9kKkjXJLkgyQ8k+WCSLyTZkuSbkrwyyR/M+zwjyeF7eL9K8pgkJyV57Lzt\n8UnOTPK8ef0bM43mnlJVP5XkhUn+JlNhOTHJ/5zbbElyb6YR4yR5x7ztkGXveX+mIrpr3zdmKr6H\nZyraj5if+96quiPJm7v7xUn+3rz9M/M+sqTt8Xv4fACwVmrzBtTmqqokl2U6Nv8nyS9W1fu6+z0P\n9joYiaDKwaSSvC/JM5L8s0zF7qlLnv9Skm9J8s1JfiPTyOX7MhWISvK4TEXqhLnN/8pUjP53kjfM\nr13qviT/NslNmaYl/fn82vuT/EmmAvlXSc5K8si5T4cluS3Ju5L8fpKfSPL2eX/vTnJbVT09UzE+\nbA+f83lV9UuZCuET5/c5NNOI8Y/Nbd6f5O8n+WRVfX+STyb522X77EzTr75QVadnmg71/iR3JXlq\nd//NHt4fABalNm9MbX5skqOSvCfJ9iQvTfKd8zocEARVDja7RmnvyjSl5jPz9mMzjVyeNG87Nskx\nmYrS0zIVv8uSHJHkHyb5nfl1D09yd6apP49Z9l5fzjRaeniS38xUCDvJ55OcOrf5jiXtb5r79bhM\no7WPyHRNzCGZRmx/OFORfsv8Xg/fw2fsTMX1e+b+XjLv675MI8LPynRdT5L8t0wF87D5ufuX7avm\n/R2aaeT6I/PnvXcP7w0AD5XavP61+Y4kf53kfyT5p/O2v9xD32BI7vrLwebwTKOx/zLT3//75uXL\nS9rck+TV8+NDM13DkkyF9AczTdvZNchzRJIPJ/nnSZ6+bHlGkl/KNHXph+b2leRbk2xLcl2ma2u+\nMD/3B0lunR9fl6kw3ZlpRPUv535s7+53d/dhma5vSabR3O3z42d398O6+9WZRnt/f+7vFUn+e5KL\nMhW975x/3tzd2zJNoXpWppHZ75z7/pFM/xH4dHf/l+6+q7uf093P6+77HuwgA8BDoDavc23u7vuT\nPGc+nlvmPr7vgYcexuWMKgebb8s04vm4JH+aqSB8NtMUoWsz3aTgZ5P8k0yjlck0Bej+TIUvSa7O\nNO0mSY7ONLL6xiQ/n6mwvS5T8bkr03UvX840IvstmUaBP5fdhezvzj+/nOn6mBMyjYIel6kgnzrv\n49i5n+9e8lnOmX9+NMkp8+P7q+qb57bPSfKieftz55+/PO/jh5Jc0d33VNUhmUZiP5Fp9Lbmtodn\nKsQ3z22+NtOI8V2Zbm4hrAKwHtTmjanNtyQ5b358TFW9pN1VmAOIM6ocbD6caWTxtkxThl6XqQh9\nONM1J6dlGt38dKZRz5sz3Vzh5EzTfD6W5PIkT6+qx2caET0rU7FLkt/s7pcmuTDTSOvN3f3O7J52\n8xeZpiJ9Yl6/N9O1NW/p7t9O8oRMo7wfT3Jkd3/D3O5j3b2lu2+oqkOr6t8nOX1+bumt8k9J8qlM\ndxe8cO5Xzz/vTfKfM02hSpJvqKpHZLohw8vmfj0zU9E/ZH58b6b/HDwu05Skk5N8V/Z8DQ4APFRq\n88bU5vOTHDk//qZM16zCAaO6e/VWsAlU1asy3aTg/ZmucXlqpqJ3VaZR2+OSHN7d51bVw5LckOmW\n8x+bX/99mQrlYZmK6IcyXZdyRaYC8vRMI66fm/f/yCSv6e5/VVWnzq99WXf/8vydbJ9IcmN3P3He\n/8sz3Yp/l7dlGun97kxF6aYk/zVTsfu1uc0fd/cPVNWVmYrXLhfNyxvmz5xM06h+OsmvZ/cdCd+W\n5B9l+o/BrtHaK+f+PyHTaHUnuW2eRgQA60ZtVpthT5xR5WDy8UwF6/5Mo5fnZhrlfGmmIvSeJM+f\nv/B7+9zu1qr67qr6k0zfc/baTNem/HamgvL5TAXn2kx38vvc/F63zNuuntc/nenOfbfM63dlmoq0\nc0n//jrTiPCrMo0kn5ipECbTzRm+bV4+OO/rTZlu4pBMN5PYdROFm5P8p/m1T8t0R8OfzjQy/aT5\nsz0j0wj0p5P8+NyPW+blW7N7atEt83NnP9iBBYC9pDarzbCiVc+oVtUbM82Zv727T1rh+UryHzJ9\nZ9UXk7yguz+4AX2Ffaaqvj7J5zfyOsyqetjejoQu+tqqesTSr5GpqkOTfLlNpYADmtrMwUhthoPL\nImdUL8l0bcCenJ7pIvMTMl2w/etr7xbsX91950bfLGgt03UWfe3y7zrt7vsUQtgULonazEFGbYaD\ny6pBtbv/LLunTKzkrCRv7slVSR5TVY9brw4CAF9NbQZgs1uPa1SPyu65/ck0Z95dxQBg/1GbATig\nrcf3qNYK21acvlBV52X+PqcjjjjiyU984hPX4e0BILnmmms+091b9nc/BqE2A7DfraU2r0dQ3Znp\ndtm7HJ3k1pUadvfFSS5Okq1bt/b27dvX4e0BIKmqT+7vPgxEbQZgv1tLbV6Pqb/bkpxbk1OS3N3d\nt63DfgGAvaM2A3BAW/WMalW9NdOXFR9ZVTuTvDLT90alu1+f6bulzkiyI9Mt8F+4UZ0FANRmADa/\nVYNqd5+zyvOd5GfWrUcAwINSmwHY7NZj6i8AAACsG0EVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMR\nVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiK\noAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQ\nBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACG\nIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAw\nFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACA\noQiqAAAADGWhoFpVp1XVjVW1o6ouWOH5Y6vqyqr6UFVdW1VnrH9XAYBd1GYANrNVg2pVHZLkoiSn\nJzkxyTlVdeKyZr+Q5LLuPjnJ2Ulet94dBQAmajMAm90iZ1SfkmRHd9/U3fckuTTJWcvadJKvmx8/\nOsmt69dFAGAZtRmATe3QBdocleSWJes7kzx1WZtXJfnjqvrZJEckOXVdegcArERtBmBTW+SMaq2w\nrZetn5Pkku4+OskZSd5SVQ/Yd1WdV1Xbq2r7HXfc8dB7CwAkajMAm9wiQXVnkmOWrB+dB04felGS\ny5Kku9+f5GuSHLl8R919cXdv7e6tW7Zs2bseAwBqMwCb2iJB9eokJ1TV8VV1WKYbMmxb1uZTSZ6d\nJFX17ZmKoWFZANgYajMAm9qqQbW770tyfpIrktyQ6Q6C11XVhVV15tzsxUl+sqo+kuStSV7Q3cun\nIAEA60BtBmCzW+RmSunuy5NcvmzbK5Y8vj7J09a3awDAnqjNAGxmi0z9BQAAgH1GUAUAAGAogioA\nAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQB\nAAAYiqAKAADAUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAK\nAADAUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARV\nAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKo\nAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRB\nFQAAgKEIqgAAAAxFUAUAAGAogioAAABDWSioVtVpVXVjVe2oqgv20ObHqur6qrquqn53fbsJACyl\nNgOwmR26WoOqOiTJRUm+P8nOJFdX1bbuvn5JmxOSvCzJ07r7zqp67EZ1GAAOdmozAJvdImdUn5Jk\nR3ff1N33JLk0yVnL2vxkkou6+84k6e7b17ebAMASajMAm9oiQfWoJLcsWd85b1vqCUmeUFV/UVVX\nVdVp69VBAOAB1GYANrVVp/4mqRW29Qr7OSHJM5McneTPq+qk7r7rq3ZUdV6S85Lk2GOPfcidBQCS\nqM0AbHKLnFHdmeSYJetHJ7l1hTbv7O57u/sTSW7MVBy/Sndf3N1bu3vrli1b9rbPAHCwU5sB2NQW\nCapXJzmhqo6vqsOSnJ1k27I270jyrCSpqiMzTTe6aT07CgB8hdoMwKa2alDt7vuSnJ/kiiQ3JLms\nu6+rqgur6sy52RVJPltV1ye5MslLuvuzG9VpADiYqc0AbHbVvfySln1j69atvX379v3y3gBsPlV1\nTXdv3d/9OJCpzQCsp7XU5kWm/gIAAMA+I6gCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMR\nVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiK\noAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQ\nBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACG\nIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAw\nFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACA\noSwUVKvqtKq6sap2VNUFD9LuR6qqq2rr+nURAFhObQZgM1s1qFbVIUkuSnJ6khOTnFNVJ67Q7lFJ\nfi7JB9a7kwDAbmozAJvdImdUn5JkR3ff1N33JLk0yVkrtHt1ktck+dI69g8AeCC1GYBNbZGgelSS\nW5as75y3fUVVnZzkmO5+1zr2DQBYmdoMwKa2SFCtFbb1V56seliSX03y4lV3VHVeVW2vqu133HHH\n4r0EAJZSmwHY1BYJqjuTHLNk/egkty5Zf1SSk5K8t6puTnJKkm0r3bShuy/u7q3dvXXLli1732sA\nOLipzQBsaosE1auTnFBVx1fVYUnOTrJt15PdfXd3H9ndx3X3cUmuSnJmd2/fkB4DAGozAJvaqkG1\nu+9Lcn6SK5LckOSy7r6uqi6sqjM3uoMAwFdTmwHY7A5dpFF3X57k8mXbXrGHts9ce7cAgAejNgOw\nmS0y9RcAAAD2GUEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgC\nAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEV\nAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiq\nAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQ\nBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiC\nKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADGWhoFpVp1XVjVW1o6ou\nWOH5n6+q66vq2qp6d1U9fv27CgDsojYDsJmtGlSr6pAkFyU5PcmJSc6pqhOXNftQkq3d/R1J3p7k\nNevdUQBgojYDsNktckb1KUl2dPdN3X1PkkuTnLW0QXdf2d1fnFevSnL0+nYTAFhCbQZgU1skqB6V\n5JYl6zvnbXvyoiR/tJZOAQAPSm0GYFM7dIE2tcK2XrFh1fOSbE3yjD08f16S85Lk2GOPXbCLAMAy\najMAm9oiZ1R3JjlmyfrRSW5d3qiqTk3y8iRndvffrrSj7r64u7d299YtW7bsTX8BALUZgE1ukaB6\ndZITqur4qjosydlJti1tUFUnJ3lDpkJ4+/p3EwBYQm0GYFNbNah2931Jzk9yRZIbklzW3ddV1YVV\ndebc7LVJvjbJ71XVh6tq2x52BwCskdoMwGa3yDWq6e7Lk1y+bNsrljw+dZ37BQA8CLUZgM1skam/\nAAAAsM8IqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRBFQAAgKEI\nqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxF\nUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAo\ngioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABD\nEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAY\niqAKAADAUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAoCwXVqjqtqm6sqh1VdcEKzx9e\nVW+bn/9AVR233h0FAHZTmwHYzFYNqlV1SJKLkpye5MQk51TVicuavSjJnd39rUl+Ncn/t94dBQAm\najMAm90iZ1SfkmRHd9/U3fckuTTJWcvanJXkTfPjtyd5dlXV+nUTAFhCbQZgU1skqB6V5JYl6zvn\nbSu26e77ktyd5BvXo4MAwAOozQBsaocu0Gal0dfeizapqvOSnDev/m1VfXSB92fPjkzymf3diQOc\nY7g+HMe1cwzX7tv2dwf2IbV5XH6X184xXB+O49o5hmu317V5kaC6M8kxS9aPTnLrHtrsrKpDkzw6\nyeeW76i7L05ycZJU1fbu3ro3nWbiGK6dY7g+HMe1cwzXrqq27+8+7ENq86Acw7VzDNeH47h2juHa\nraU2LzL19+okJ1TV8VV1WJKzk2xb1mZbkufPj38kyXu6+wGjtgDAulCbAdjUVj2j2t33VdX5Sa5I\nckiSN3b3dVV1YZLt3b0tyW8leUtV7cg0Wnv2RnYaAA5majMAm90iU3/T3ZcnuXzZtlcsefylJD/6\nEN/74ofYngdyDNfOMVwfjuPaOYZrd1AdQ7V5WI7h2jmG68NxXDvHcO32+hiWWUAAAACMZJFrVAEA\nAGCf2fCgWlWnVdWNVbWjqi5Y4fnDq+pt8/MfqKrjNrpPB5oFjuHPV9X1VXVtVb27qh6/P/o5stWO\n4ZJ2P1JVXVXu8LbMIsewqn5s/rt4XVX97r7u4+gW+F0+tqqurKoPzb/PZ+yPfo6sqt5YVbfv6StU\navIf52N8bVV9177u44FAbV47tXnt1Oa1U5vXTm1euw2rzd29YUumGzz8dZJvSXJYko8kOXFZm59O\n8vr58dlJ3raRfTrQlgWP4bOSPHJ+/FOO4UM/hnO7RyX5syRXJdm6v/s90rLg38MTknwoydfP64/d\n3/0eaVnwGF6c5KfmxycmuXl/93u0Jcn3JfmuJB/dw/NnJPmjTN8hekqSD+zvPo+2qM377BiqzWs8\nhnM7tXkNx1BtXpdjqDavfhw3pDZv9BnVpyTZ0d03dfc9SS5NctayNmcledP8+O1Jnl1VK31J+cFq\n1WPY3Vd29xfn1asyfZ8euy3y9zBJXp3kNUm+tC87d4BY5Bj+ZJKLuvvOJOnu2/dxH0e3yDHsJF83\nP350Hvi9mAe97v6zrPBdoEucleTNPbkqyWOq6nH7pncHDLV57dTmtVOb105tXju1eR1sVG3e6KB6\nVJJblqzvnLet2Ka770tyd5Jv3OB+HUgWOYZLvSjTiAW7rXoMq+rkJMd097v2ZccOIIv8PXxCkidU\n1V9U1VVVddo+692BYZFj+Kokz6uqnZnu5vqz+6Zrm8pD/TfzYKQ2r53avHZq89qpzWunNu8be1Wb\nF/p6mjVYafR1+W2GF2lzMFv4+FTV85JsTfKMDe3RgedBj2FVPSzJryZ5wb7q0AFokb+Hh2aaYvTM\nTGcO/ryqTuruuza4bweKRY7hOUku6e5fqarvyfQdmCd19/0b371NQ01Zndq8dmrz2qnNa6c2r53a\nvG/sVU3Z6DOqO5Mcs2T96DzwdPlX2lTVoZlOqT/YqeODzSLHMFV1apKXJzmzu/92H/XtQLHaMXxU\nkpOSvLeqbs40d36bmzZ8lUV/l9/Z3fd29yeS3JipODJZ5Bi+KMllSdLd70/yNUmO3Ce92zwW+jfz\nIKc2r53avHZq89qpzWunNu8be1WbNzqoXp3khKo6vqoOy3RDhm3L2mxL8vz58Y8keU/PV92SZIFj\nOE+NeUOmQujagwd60GPY3Xd395HdfVx3H5fpWqIzu3v7/unukBb5XX5HppuHpKqOzDTd6KZ92sux\nLXIMP5Xk2UlSVd+eqRjesU97eeDbluTc+Q6DpyS5u7tv29+dGozavHZq89qpzWunNq+d2rxv7FVt\n3tCpv919X1Wdn+SKTHfVemN3X1dVFybZ3t3bkvxWplPoOzKN1p69kX060Cx4DF+b5GuT/N58r4tP\ndfeZ+63Tg1nwGPIgFjyGVyT5f6rq+iRfTvKS7v7s/uv1WBY8hi9O8htV9f9mmhLzAuHgq1XVWzNN\nYTtyvl7olUkeniTd/fpM1w+dkWRHki8meeH+6em41Oa1U5vXTm1eO7V57dTm9bFRtbkcZwAAAEay\n0VN/AQAA4CERVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAA\nAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEA\nABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoA\nAMBQBFUAAACGIqgCAAAwFEEVAACAoRy6vzsA+0tVPSbJB5L8fnf/6yXbvynJP0jyPUl+OMnrkzwq\nyQuSXJLkI0n+sLtvfwjv9TNJfjHJM7r7uqp6ZJJrk1zT3T++pN1xSa5K8sLu/qMH2d/fSfLcPPB3\n+P4k7+7u65a0fWWSJ3X3c+f1Nyf5q+5+7aL9B4CNoh6rx7Ci7rZYDoolU7H7UpJOcnuSK+fHdyb5\nN0n+8dzuxfP2pcvHk/xekrcm+VySlyxp+6UV2nemIvUrSX4wyafnbTcn+dEkvzavfybJK5I8ed7f\n6+ft5yd557L9fTHJc+Z2v7CH9+wkl85tvj3JLUnOTfJzSa5P8vy5zZ8uOzZvmrdfn+Sj8/Kxedtv\nLXh8X7WkD7+2v/+8LRaLxTLmoh5veD1+79z+uCXbXj1/xk8kOWt//x2wWBZZnFHlYPKkJIdkd9F6\ncpLPJqkkL09yT1X9TqbidE2SC5L8QJIPJvlCki1JvinJK5P8wbzPM5Icvof3qySPSXJSksfO2x6f\n5Mwkz5vXvzHTyO4pVfVTSV6Y5G8yFagTk/zPuc2WJPdmGj1OknfM2w5Z9p73Zyqou/Z9Y6ZCfHim\nAv6I+bnvrao7kry5u1+c5O/N2z8z7yNL2h6/h8+3+4NWVZKfWK0dAEQ93rB6vJKqekamQH1XkqOS\nvKmqju7u/7M3+4N9RVDlYFJJ3pfkGUn+WabC99Qlz38pybck+eYkv5FpFPN9mYpFJXlcpoJ1wtzm\nf2UqTP87yRvm1y51X5J/m+SmTFOU/nx+7f1J/iRTsfyrJGcleeTcp8OS3JbkXUl+P1P4e/u8v3cn\nua2qnp6pMB+2h8/5vKr6pUxF8Ynz+xyaafT4x+Y270/y95N8sqq+P8knk/ztsn12pqlYX6iq0zNN\njXp/pkL31O7+myVtnz5//vck+b/30C8ASNTjm7Nx9XglT5l/vjTJs5Kck+ks79WrvA72K0GVg82u\nEdu7Mk2v+cy8/dhMo5gnzduOTXJMpgL1tEyF8LIkRyT5h0l+Z37dw5PcnWka0GOWvdeXM42cHp7k\nNzMVxU7y+SSnzm2+Y0n7m+Z+PS7TyO0jMl0fc0im0dsfzlSw3zK/18P38Bk7U6H9nrm/l8z7ui/T\n6PCzMl3jkyT/LVPxPGx+7v5l+6p5f4dmGsX+yPx5713W7vnza38ngioAq1OPN6Yer+SI+ee9Se5Z\ntg2GJajc1rH7AAAgAElEQVRysDk808jsv0zyR5mKRTIVsV3uyXQtxy9k+h35dKbC+NnsHgHd9btz\nRJIPJ/nn2T01Z6lfmt/vh+b1SvKtSbYl+b8yjQifMT/3B0n+Raaid12mInVnpoK0c37d9u5+d5LD\nquonkrw508juo5NsTfLs7n5PklTVyZmmRn1jkj+c9/eRTDeh+M5MhfDm7v5YVX33/Hl/NdM1NI9O\n8h+T/EySt3T3f5n7+JzlH7CqHpFpOtPDMk3nAoDVqMfrXI8fxBfmnw/P7jO1X9hDWxiGoMrB5tsy\nFZ7HJfnTTMXhs5mmC12b6YYFP5vkn2QauUym6UD3Z/fo49WZpuAkydGZRlnfmOTnM/3D/7okV2Qa\njX18pqJ7U6apSO/KdPOHc+fX/93555czXStzQpI7khyXqTifOu/j2Lmf717yWc6Zf340ySnz4/ur\nalcRf06SF83bnzv//OV5Hz+U5IruvqeqDsk0KvuJTAWs5raHZ5qCdPPc5mszjR7flelGF7v+U/E1\nmc6oJiv/5wAAllOP178e78muKb6vyXTs7k5ywyqvgf3O96hysPlwppse3JZpVPZ1mQrShzNdf3Ja\nkv+eadT27ZmuI3lekpMzTfn5WJLLkzy9qh6faXT0rEyFL0l+s7tfmuTCJH+ZaYT0nUn+6fz8X2Sa\nlvSJef3eTKO4b+nu307yhEw3kvh4kiO7+xvmdh/r7i3dfUNVHVpV/z7J6fNzS2+bf0qST2W60+CF\nc796/nlvkv+caTpVknzDfDb0+CQvm/v1zExF7JD58b2Z/qPwuEzTk05O8l1Zcu1Md9/Z3e/o7nfM\nxxUAVqMer3M93pPufm+Sf5cp5N+a5Fw3UuJAUN29eivYBKrqVZluWPD+TNe7PDVTAbwq0wjucUkO\n7+5zq+phmUYbn9PdH5tf/32ZiuZhmQrqhzJdo3JFpmLy9Eyjr5+b9//IJK/p7n9VVafOr31Zd//y\n/P1sn0hyY3c/cd7/yzPdln+Xt2Ua9f3uTAXqpiT/NVPh+7W5zR939w9U1ZWZCtkuF83LG+bPnEzT\nqn46ya9n990J35bkH2X6T8Kukdsr5/4/IVNR6yS3dffy62UA4CFTj9VjWIQzqhxMPp6peN2faSTz\n3Ewjni/NVJDek+T585d/b5/b3VpV311Vf5JpeutrM12n8tuZisvnMxWfazPd1e9z83vdMm/bNd3m\n05nu4nfLvH5Xdl/rsstfZxodflWmUeUTMxXFZLqu5Nvm5YPzvt6U6YYOyXRjiV03VLg5yX+aX/u0\nTHc3/OlMo9RPmj/bMzKNRn86yY/P/bhlXr41u6cZ3TI/d/aDHVgAeAjUY/UYVrXqGdWqemOm+fO3\nd/dJKzxfSf5DpgvQv5jkBd39wQ3oK+wzVfX1ST6/wHUfa3mPh+3tqOiir62qRyy9bX1VHZrky20q\nBRzQ1GYOFuoxHLwWOaN6SabrBPbk9EwXnJ+Q5LxM0xjggDZfd7lhRXF+j72eurPoa5d/t1p336co\nwqZwSdRmDgLqMRy8Vg2q3f1n2T19YiVnJXlzT65K8piqetx6dRAA+GpqMwCb3Xpco3pUds/zT6b5\n80etw34BgL2jNgNwQFuP71GtFbatOJWhqs7LNAUpRxxxxJOf+MQnrsPbA0ByzTXXfKa7t+zvfgxC\nbQZgv1tLbV6PoLoz062zdzk603c0PUB3X5zk4iTZunVrb9++fR3eHgCSqvrk/u7DQNRmAPa7tdTm\n9Zj6uy3JuTU5Jcnd3X3bOuwXANg7ajMAB7RVz6hW1VszfXHxkVW1M8krM32HVLr79Zm+Z+qMJDsy\n3QL/hRvVWQBAbQZg81s1qHb3Oas830l+Zt16BAA8KLUZgM1uPab+AgAAwLoRVAEAABiKoAoAAMBQ\nBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACG\nIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAw\nFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACA\noQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAA\nDEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAA\nYCiCKgAAAEMRVAEAABiKoAoAAMBQFgqqVXVaVd1YVTuq6oIVnj+2qq6sqg9V1bVVdcb6dxUA2EVt\nBmAzWzWoVtUhSS5KcnqSE5OcU1UnLmv2C0ku6+6Tk5yd5HXr3VEAYKI2A7DZLXJG9SlJdnT3Td19\nT5JLk5y1rE0n+br58aOT3Lp+XQQAllGbAdjUDl2gzVFJblmyvjPJU5e1eVWSP66qn01yRJJT16V3\nAMBK1GYANrVFzqjWCtt62fo5SS7p7qOTnJHkLVX1gH1X1XlVtb2qtt9xxx0PvbcAQKI2A7DJLRJU\ndyY5Zsn60Xng9KEXJbksSbr7/Um+JsmRy3fU3Rd399bu3rply5a96zEAoDYDsKktElSvTnJCVR1f\nVYdluiHDtmVtPpXk2UlSVd+eqRgalgWAjaE2A7CprRpUu/u+JOcnuSLJDZnuIHhdVV1YVWfOzV6c\n5Cer6iNJ3prkBd29fAoSALAO1GYANrtFbqaU7r48yeXLtr1iyePrkzxtfbsGAOyJ2gzAZrbI1F8A\nAADYZwRVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARV\nAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKo\nAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRB\nFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRBFQAAgKEI\nqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxF\nUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMJSFgmpVnVZVN1bVjqq6YA9tfqyq\nrq+q66rqd9e3mwDAUmozAJvZoas1qKpDklyU5PuT7ExydVVt6+7rl7Q5IcnLkjytu++sqsduVIcB\n4GCnNgOw2S1yRvUpSXZ0903dfU+SS5OctazNTya5qLvvTJLuvn19uwkALKE2A7CpLRJUj0pyy5L1\nnfO2pZ6Q5AlV9RdVdVVVnbZeHQQAHkBtBmBTW3Xqb5JaYVuvsJ8TkjwzydFJ/ryqTuruu75qR1Xn\nJTkvSY499tiH3FkAIInaDMAmt8gZ1Z1JjlmyfnSSW1do887uvre7P5HkxkzF8at098XdvbW7t27Z\nsmVv+wwABzu1GYBNbZGgenWSE6rq+Ko6LMnZSbYta/OOJM9Kkqo6MtN0o5vWs6MAwFeozQBsaqsG\n1e6+L8n5Sa5IckOSy7r7uqq6sKrOnJtdkeSzVXV9kiuTvKS7P7tRnQaAg5naDMBmV93LL2nZN7Zu\n3drbt2/fL+8NwOZTVdd099b93Y8DmdoMwHpaS21eZOovAAAA7DOCKgAAAEMRVAEAABiKoAoAAMBQ\nBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACG\nIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAw\nFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACA\noQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAA\nDEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAA\nYCiCKgAAAEMRVAEAABjKQkG1qk6rqhurakdVXfAg7X6kqrqqtq5fFwGA5dRmADazVYNqVR2S5KIk\npyc5Mck5VXXiCu0eleTnknxgvTsJAOymNgOw2S1yRvUpSXZ0903dfU+SS5OctUK7Vyd5TZIvrWP/\nAIAHUpsB2NQWCapHJbllyfrOedtXVNXJSY7p7netY98AgJWpzQBsaosE1VphW3/lyaqHJfnVJC9e\ndUdV51XV9qrafscddyzeSwBgKbUZgE1tkaC6M8kxS9aPTnLrkvVHJTkpyXur6uYkpyTZttJNG7r7\n4u7e2t1bt2zZsve9BoCDm9oMwKa2SFC9OskJVXV8VR2W5Owk23Y92d13d/eR3X1cdx+X5KokZ3b3\n9g3pMQCgNgOwqa0aVLv7viTnJ7kiyQ1JLuvu66rqwqo6c6M7CAB8NbUZgM3u0EUadfflSS5ftu0V\ne2j7zLV3CwB4MGozAJvZIlN/AQAAYJ8RVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiq\nAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQ\nBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiC\nKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMR\nVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiK\noAoAAMBQBFUAAACGIqjy/7d399GW3eV92L8PEpLNi8FBg0P1gkQtjKeqbfBElmPXwEJJJDVIjRe2\npVblpaqVYstJA8WVSxYQkbQJLBc3iRyQA+ElNULg1EypqNyAHGwXEQ0GFCQqexAymoqFBAjVtgJ6\ne/rH3mKu7tzRHM25L7+58/msddacs89v9n3ub907z3z3/u19AAAAhiKoAgAAMBRBFQAAgKEIqgAA\nAAxFUAUAAGAogioAAABDEVQBAAAYykJBtarOqapbq2pvVV2+xvuvqapbquqmqvpYVT17/UsFAB6h\nNwOwnR0yqFbVMUmuTHJukp1JLqqqnauGfSbJru7+oSQfSvKW9S4UAJjozQBsd4ucUT0zyd7uvq27\n709ydZILVg7o7uu7+7755Q1JTlrfMgGAFfRmALa1RYLqiUnuWPF637ztYC5J8tFligIAHpPeDMC2\nduwCY2qNbb3mwKqLk+xK8sKDvH9pkkuT5JRTTlmwRABgFb0ZgG1tkTOq+5KcvOL1SUnuXD2oqs5O\n8vok53f3t9faUXdf1d27unvXjh07DqdeAEBvBmCbWySo3pjk9Ko6raqOS3Jhkt0rB1TV85O8I1Mj\nvGv9ywQAVtCbAdjWDhlUu/vBJJcluS7JF5Jc0903V9UVVXX+POytSZ6S5INV9dmq2n2Q3QEAS9Kb\nAdjuFrlGNd19bZJrV217w4rnZ69zXQDAY9CbAdjOFln6CwAAAJtGUAUAAGAogioAAABDEVQBAAAY\niqAKAADAUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADA\nUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAA\nhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAA\nMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRBFQAA\ngKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAA\nAAxFUAUAAGAogioAAABDWSioVtU5VXVrVe2tqsvXeP/4qvrA/P6nqurU9S4UANhPbwZgOztkUK2q\nY5JcmeTcJDuTXFRVO1cNuyTJPd39/UneluQfrXehAMBEbwZgu1vkjOqZSfZ2923dfX+Sq5NcsGrM\nBUneMz//UJKXVFWtX5kAwAp6MwDb2iJB9cQkd6x4vW/etuaY7n4wyb1JnrEeBQIAB9CbAdjWjl1g\nzFpHX/swxqSqLk1y6fzy21X1+QW+Pgd3QpKvbXURRzhzuD7M4/LM4fJ+YKsL2ER687j8Li/PHK4P\n87g8c7i8w+7NiwTVfUlOXvH6pCR3HmTMvqo6NsnTknxj9Y66+6okVyVJVe3p7l2HUzQTc7g8c7g+\nzOPyzOHyqmrPVtewifTmQZnD5ZnD9WEel2cOl7dMb15k6e+NSU6vqtOq6rgkFybZvWrM7iSvmJ+/\nLMnHu/uAo7YAwLrQmwHY1g55RrW7H6yqy5Jcl+SYJO/q7pur6ooke7p7d5J3JnlfVe3NdLT2wo0s\nGgCOZnozANvdIkt/093XJrl21bY3rHj+rSQ/8zi/9lWPczwHMofLM4frwzwuzxwu76iaQ715WOZw\neeZwfZjH5ZnD5R32HJZVQAAAAIxkkWtUAQAAYNNseFCtqnOq6taq2ltVl6/x/vFV9YH5/U9V1akb\nXdORZoE5fE1V3VJVN1XVx6rq2VtR58gONYcrxr2sqrqq3OFtlUXmsKp+dv5ZvLmqfnOzaxzdAr/L\np1TV9VX1mfn3+bytqHNkVfWuqrrrYB+hUpN/PM/xTVX1gs2u8UigNy9Pb16e3rw8vXl5evPyNqw3\nd/eGPTLd4OGLSZ6T5Lgkn0uyc9WYX0jy9vn5hUk+sJE1HWmPBefwxUmeND9/tTl8/HM4j3tqkk8k\nuSHJrq2ue6THgj+Hpyf5TJLvnV8/c6vrHumx4BxeleTV8/OdSW7f6rpHeyT5qSQvSPL5g7x/XpKP\nZvoM0bOSfGqrax7toTdv2hzqzUvO4TxOb15iDvXmdZlDvfnQ87ghvXmjz6iemWRvd9/W3fcnuTrJ\nBavGXJDkPfPzDyV5SVWt9SHlR6tDzmF3X9/d980vb8j0eXrst8jPYZK8OclbknxrM4s7Qiwyhz+f\n5MruvidJuvuuTa5xdIvMYSf5nvn503Lg52Ie9br7E1njs0BXuCDJe3tyQ5KnV9WzNqe6I4bevDy9\neXl68/L05uXpzetgo3rzRgfVE5PcseL1vnnbmmO6+8Ek9yZ5xgbXdSRZZA5XuiTTEQv2O+QcVtXz\nk5zc3R/ZzMKOIIv8HD43yXOr6g+q6oaqOmfTqjsyLDKHb0pycVXty3Q311/anNK2lcf7b+bRSG9e\nnt68PL15eXrz8vTmzXFYvXmhj6dZwlpHX1ffZniRMUezheenqi5OsivJCze0oiPPY85hVT0hyduS\nvHKzCjoCLfJzeGymJUYvynTm4Peq6ozu/uYG13akWGQOL0ry7u7+1ar68UyfgXlGdz+88eVtG3rK\noenNy9Obl6c3L09vXp7evDkOq6ds9BnVfUlOXvH6pBx4uvw7Y6rq2Eyn1B/r1PHRZpE5TFWdneT1\nSc7v7m9vUm1HikPN4VOTnJHkd6vq9kxr53e7acOjLPq7/OHufqC7v5Tk1kzNkckic3hJkmuSpLs/\nmeS7kpywKdVtHwv9m3mU05uXpzcvT29ent68PL15cxxWb97ooHpjktOr6rSqOi7TDRl2rxqzO8kr\n5ucvS/Lxnq+6JckCczgvjXlHpkbo2oMDPeYcdve93X1Cd5/a3admupbo/O7eszXlDmmR3+XfznTz\nkFTVCZmWG922qVWObZE5/HKSlyRJVf1gpmZ496ZWeeTbneTl8x0Gz0pyb3d/ZauLGozevDy9eXl6\n8/L05uXpzZvjsHrzhi797e4Hq+qyJNdluqvWu7r75qq6Isme7t6d5J2ZTqHvzXS09sKNrOlIs+Ac\nvjXJU5J8cL7XxZe7+/wtK3owC84hj2HBObwuyV+tqluSPJTkdd399a2reiwLzuFrk/xGVf2dTEti\nXikcPFpVvT/TErYT5uuF3pjkiUnS3W/PdP3QeUn2Jrkvyau2ptJx6c3L05uXpzcvT29ent68Pjaq\nN5d5BgAAYCQbvfQXAAAAHhdBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARV\nAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKo\nAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRB\nFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYyrFbXQBspqp6epJPJfmt7v4fVmz/viT/WZIf\nT/I3krw9yVOTvDLJu5N8Lsn/1t13PY6v9YtJ/l6SF3b3zVX1pCQ3Jfl0d//cinGnJrkhyau6+6OP\nsb//IMlP58Df24eTfKy7b14x9o1Jfri7f3p+/d4k/66737po/QCwGfRmvRnW1N0eHtv2kanBfStJ\nJ7kryfXz83uS/P0k/9U87rXz9pWPP07ywSTvT/KNJK9bMfZba4zvTI3pV5P8p0m+Om+7PcnPJPm1\n+fXXkrwhyY/O+3v7vP2yJB9etb/7krx0Hvd3D/I1O8nV85gfTHJHkpcn+VtJbknyinnMv1k1N++Z\nt9+S5PPz44/mbe9cYG7/8vx3v57kDSu2v3n+Hr+U5IKt/hnw8PDw8BjroTdvaG/+D1fV8PR5+6uT\nfGV+/M2t/hnw8Fjk4Ywq290PJzkm+xvVj2YKVpXk9Unur6p/makhfTrJ5Un+WpI/TPLnSXYk+b4k\nb0zyr+Z9npfk+IN8vUry9CRnJHnmvO3ZSc5PcvH8+hmZjuaeVVWvTvKqJP8+U1PameT/mcfsSPJA\npiPGSfLb87ZjVn3NhzM10Uf2fWum5nt8pqb93fN7f7mq7k7y3u5+bZL/eN7+tXkfWTH2tIN8f9M3\nWVVJrsk0N3+W5O9V1e8neShT0/5mkhOTvKeqTuruP3us/QFwVNGbN6A3z86c/9yXaU4fqqrnJPkn\nSb49v3dlVf1Od39pgf3BlhFU2e4qye8neWGS/yZTs/uxFe9/K8lzkvzFJL+R6cjl72dqEJXkWZma\n1OnzmP83UzP6/5K8Y/67Kz2Y5H9MclumZUm/N//dh5P860wN8t8luSDJk+aajst0hPMjSX4ryX+Z\n5EPz/j6W5CtV9ZOZmvFxB/k+L66qf5CpET5v/jrHZjpi/LPzmE8m+U+S/ElV/ZUkf5Kpaa3cZ2da\nfvXnVXVupuVQn8wUPH+su//9PO6ZmYLox5PsSfLLSX4k+xv1Lyd5cZKLMh1JvvEgdQNw9NGbN6Y3\nJ/uD6tcynTn906o6Z56fX5vn71eS7Mq08gmGJahyNHjkKO03My2p+dq8/ZRMRy7PmLedkuTkTE3p\nJzI1v2uSPDnJf57kX85/74lJ7s209Ofpq77WQ5mOlh6f5J9naoSd5E+TnD2P+aEV42+b63pWpqO1\n353pmphjMh2x/RuZmvT75q/1xIN8j52puf74XO+75309mOmI8IszXdeTJP9npoZ53Pzew6v2VfP+\njs105Ppz8/f7wIoxdyf5YpL/O8nfnLf92xXf4wNJ7p+fP/kgNQNw9NKb1783J9My6oeT/LdJrq2q\nZ2d/H145Vm9meO76y9Hg+ExHY/+7TD/zD86Ph1aMuT/TtZXJ1AS+Oj//eqZrWp6R/Qd2npzks0n+\ndpKfXPV4YZJ/kGnp0l+fx1eS70+yO8nNma6t+fP5vX+V5M75+c2Zmss9mY6o/tu5jj3d/bHuPi7T\n9S3JdDR3z/z8Jd39hO5+c6ajvb8113tdkv8ryZWZmt6PzH/e3t27My2henGmI7M/Mtf+uUz/Efhq\nd//v3f3N7n5pd1/c3Q8+Mlnd/XCSl87zuWOu8fdXfF9PzP6jwY9sA4BH6M3r3JtnH5mXEO+dv97O\n6M0coZxR5WjwA5mOeD4ryb/J1BC+nmmJ0E2ZblLwS0n+60xHK5NpCdDD2X/E8cZMy26S5KRMR1bf\nleQ1mf6x//VMzeebma57eSjTEdnnZDoK/I3sb2T/0fznQ5mujzk90xnKUzM15LPnfZwy1/mxFd/L\nRfOfn09y1vz84ar6i/PYlya5ZN7+0/Of/3Dex19Pcl13319Vx2Q6EvulTE2r5rHHZ2rEt89jnpLp\niPE3M93cYmVDvCPJpfPzk6vqddm/xPct89zdm+QLAYBH05s3pjfvrqofnufjvvn7/cY8b3973udD\n2R+oYVjOqHI0+Gyms35fybRk6NczNaHPZrrm5JxMRze/mumo5+2Zbq7w/EzLfP4oybVJfnJeQvOM\nTNex3Dbv/5939y8nuSLTkdbbu/vD2b8k9g8yNYtHrgV5INO1Ne/r7n+R5LmZjvL+cZITuvsvzOP+\nqLt3dPcXqurYqvqfk5w7v7fyVvlnJflyprsLXjHX1fOfDyT5XzMtoUqSv1BV353phgy/Mtf1okxN\n/5j5+QOZ/nPwrExLkp6f5AU58Bqcy5KcMD//viQndvfvJvmfMjXEO5O83I2UAFiD3rwxvfmOeS7+\nOMnPdffXu/uLmULqn2U6EPCLbqTEkaC6+9Cj4AhVVW/KdJOCT2a6xuXHMjW9GzL9Y31qkuO7++VV\n9YRMZ/9e2t1/NP/9n8rUKI/L1EQ/k+m6lOsyNZCfzHTE9Rvz/p+U5C3d/d9X1dnz3/2V7v6H82ey\nfRtH5BMAABUeSURBVCnJrd39vHn/r890K/5HfCDTkd6/lKkp3Zbk/8jU7H5tHvM73f3Xqur6TM3r\nEVfOj3fM33MyLaP6hST/LPtvdPSBJP9Fpv8YPHK09vq5/udmCpmd5CvzEl8AWDd6s94Mi3BGle3u\njzM1rIczHb18eaajnL+cqQl9PMkr5g/83jOPu7Oq/lJV/etMn3P21kzXpvyLTA3lTzM1nJsy3cnv\nG/PXumPe9sjy169munPfHfPrb2ZairRvRX1fzHRE+E2ZjiTvzNQIk+lakh+YH3847+s9mW7ikEw3\nk3jkxgi3J/mn89/9iUx3NPyFTEemf3j+3l6Y6Qj0V5P83FzHHfPj+7N/adEd83sXPtbEAsBh0pv1\nZjikQ55Rrap3ZVo/f1d3n7HG+5Xkf8n0+VX3JXlld//hBtQKG6qqvjfJn65xY4L1/BpPONwjoYv+\n3ar67pW3qq+qY5M81JZPwLahN3O00Jvh6LXIGdV3Z7pO4GDOzXTB+emZbqzyz5YvCzZfd9+zkY1w\n/hqHvVxn0b+76vPU0t0PaoSw7bw7ejNHAb0Zjl6HDKrd/YnsXz6xlguSvLcnNyR5elU9a70KBAAe\nTW8GYLtbj2tUT8z+df7JtH7+xHXYLwBwePRmAI5o6/E5qrXGtjWXMlTVpZk/d/HJT37yjz7vec9b\nhy8PAMmnP/3pr3X3jq2uYxB6MwBbbpnevB5BdV+mW2c/4qRMn594gO6+KslVSbJr167es8dnDQOw\nPqrqT7a6hoHozQBsuWV683os/d2d5OU1OSvJvd39lXXYLwBwePRmAI5ohzyjWlXvz/TBxSdU1b4k\nb8z0GVLp7rdn+pyp85LszXQL/FdtVLEAgN4MwPZ3yKDa3Rcd4v1O8ovrVhEA8Jj0ZgC2u/VY+gsA\nAADrRlAFAABgKIIqAAAAQxFUAQAAGIqgCgAAwFAEVQAAAIYiqAIAADAUQRUAAIChCKoAAAAMRVAF\nAABgKIIqAAAAQxFUAQAAGIqgCgAAwFAEVQAAAIYiqAIAADAUQRUAAIChCKoAAAAMRVAFAABgKIIq\nAAAAQxFUAQAAGIqgCgAAwFAEVQAAAIYiqAIAADAUQRUAAIChCKoAAAAMRVAFAABgKIIqAAAAQxFU\nAQAAGIqgCgAAwFAEVQAAAIYiqAIAADAUQRUAAIChCKoAAAAMRVAFAABgKIIqAAAAQxFUAQAAGIqg\nCgAAwFAEVQAAAIYiqAIAADAUQRUAAIChCKoAAAAMRVAFAABgKIIqAAAAQxFUAQAAGIqgCgAAwFAE\nVQAAAIYiqAIAADAUQRUAAIChCKoAAAAMRVAFAABgKIIqAAAAQ1koqFbVOVV1a1XtrarL13j/lKq6\nvqo+U1U3VdV5618qAPAIvRmA7eyQQbWqjklyZZJzk+xMclFV7Vw17O8muaa7n5/kwiS/vt6FAgAT\nvRmA7W6RM6pnJtnb3bd19/1Jrk5ywaoxneR75udPS3Ln+pUIAKyiNwOwrR27wJgTk9yx4vW+JD+2\nasybkvxOVf1SkicnOXtdqgMA1qI3A7CtLXJGtdbY1qteX5Tk3d19UpLzkryvqg7Yd1VdWlV7qmrP\n3Xff/firBQASvRmAbW6RoLovyckrXp+UA5cPXZLkmiTp7k8m+a4kJ6zeUXdf1d27unvXjh07Dq9i\nAEBvBmBbWySo3pjk9Ko6raqOy3RDht2rxnw5yUuSpKp+MFMzdFgWADaG3gzAtnbIoNrdDya5LMl1\nSb6Q6Q6CN1fVFVV1/jzstUl+vqo+l+T9SV7Z3auXIAEA60BvBmC7W+RmSunua5Ncu2rbG1Y8vyXJ\nT6xvaQDAwejNAGxniyz9BQAAgE0jqAIAADAUQRUAAIChCKoAAAAMRVAFAABgKIIqAAAAQxFUAQAA\nGIqgCgAAwFAEVQAAAIYiqAIAADAUQRUAAIChCKoAAAAMRVAFAABgKIIqAAAAQxFUAQAAGIqgCgAA\nwFAEVQAAAIYiqAIAADAUQRUAAIChCKoAAAAMRVAFAABgKIIqAAAAQxFUAQAAGIqgCgAAwFAEVQAA\nAIYiqAIAADAUQRUAAIChCKoAAAAMRVAFAABgKIIqAAAAQxFUAQAAGIqgCgAAwFAEVQAAAIYiqAIA\nADAUQRUAAIChCKoAAAAMRVAFAABgKIIqAAAAQxFUAQAAGIqgCgAAwFAEVQAAAIYiqAIAADAUQRUA\nAIChCKoAAAAMRVAFAABgKIIqAAAAQxFUAQAAGIqgCgAAwFAEVQAAAIYiqAIAADAUQRUAAIChLBRU\nq+qcqrq1qvZW1eUHGfOzVXVLVd1cVb+5vmUCACvpzQBsZ8ceakBVHZPkyiR/Jcm+JDdW1e7uvmXF\nmNOT/EqSn+jue6rqmRtVMAAc7fRmALa7Rc6onplkb3ff1t33J7k6yQWrxvx8kiu7+54k6e671rdM\nAGAFvRmAbW2RoHpikjtWvN43b1vpuUmeW1V/UFU3VNU561UgAHAAvRmAbe2QS3+T1Brbeo39nJ7k\nRUlOSvJ7VXVGd3/zUTuqujTJpUlyyimnPO5iAYAkejMA29wiZ1T3JTl5xeuTkty5xpgPd/cD3f2l\nJLdmao6P0t1Xdfeu7t61Y8eOw60ZAI52ejMA29oiQfXGJKdX1WlVdVySC5PsXjXmt5O8OEmq6oRM\ny41uW89CAYDv0JsB2NYOGVS7+8EklyW5LskXklzT3TdX1RVVdf487LokX6+qW5Jcn+R13f31jSoa\nAI5mejMA2111r76kZXPs2rWr9+zZsyVfG4Dtp6o+3d27trqOI5neDMB6WqY3L7L0FwAAADaNoAoA\nAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUA\nAACGIqgCAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgC\nAAAwFEEVAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEV\nAACAoQiqAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiq\nAAAADEVQBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGIqgCAAAwFEEVAACAoQiqAAAADEVQ\nBQAAYCiCKgAAAEMRVAEAABiKoAoAAMBQBFUAAACGslBQrapzqurWqtpbVZc/xriXVVVX1a71KxEA\nWE1vBmA7O2RQrapjklyZ5NwkO5NcVFU71xj31CR/K8mn1rtIAGA/vRmA7W6RM6pnJtnb3bd19/1J\nrk5ywRrj3pzkLUm+tY71AQAH0psB2NYWCaonJrljxet987bvqKrnJzm5uz+yjrUBAGvTmwHY1hYJ\nqrXGtv7Om1VPSPK2JK895I6qLq2qPVW15+677168SgBgJb0ZgG1tkaC6L8nJK16flOTOFa+fmuSM\nJL9bVbcnOSvJ7rVu2tDdV3X3ru7etWPHjsOvGgCObnozANvaIkH1xiSnV9VpVXVckguT7H7kze6+\nt7tP6O5Tu/vUJDckOb+792xIxQCA3gzAtnbIoNrdDya5LMl1Sb6Q5Jruvrmqrqiq8ze6QADg0fRm\nALa7YxcZ1N3XJrl21bY3HGTsi5YvCwB4LHozANvZIkt/AQAAYNMIqgAAAAxFUAUAAGAogioAAABD\nEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAY\niqAKAADAUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADA\nUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAA\nhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAA\nMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRBFQAA\ngKEIqgAAAAxFUAUAAGAoCwXVqjqnqm6tqr1Vdfka77+mqm6pqpuq6mNV9ez1LxUAeITeDMB2dsig\nWlXHJLkyyblJdia5qKp2rhr2mSS7uvuHknwoyVvWu1AAYKI3A7DdLXJG9cwke7v7tu6+P8nVSS5Y\nOaC7r+/u++aXNyQ5aX3LBABW0JsB2NYWCaonJrljxet987aDuSTJR5cpCgB4THozANvasQuMqTW2\n9ZoDqy5OsivJCw/y/qVJLk2SU045ZcESAYBV9GYAtrVFzqjuS3LyitcnJblz9aCqOjvJ65Oc393f\nXmtH3X1Vd+/q7l07duw4nHoBAL0ZgG1ukaB6Y5LTq+q0qjouyYVJdq8cUFXPT/KOTI3wrvUvEwBY\nQW8GYFs7ZFDt7geTXJbkuiRfSHJNd99cVVdU1fnzsLcmeUqSD1bVZ6tq90F2BwAsSW8GYLtb5BrV\ndPe1Sa5dte0NK56fvc51AQCPQW8GYDtbZOkvAAAAbBpBFQAAgKEIqgAAAAxFUAUAAGAogioAAABD\nEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAY\niqAKAADAUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADA\nUARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAA\nhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAA\nMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYiqAKAADAUARVAAAAhiKoAgAAMBRBFQAA\ngKEIqgAAAAxloaBaVedU1a1VtbeqLl/j/eOr6gPz+5+qqlPXu1AAYD+9GYDt7JBBtaqOSXJlknOT\n7ExyUVXtXDXskiT3dPf3J3lbkn+03oUCABO9GYDtbpEzqmcm2dvdt3X3/UmuTnLBqjEXJHnP/PxD\nSV5SVbV+ZQIAK+jNAGxriwTVE5PcseL1vnnbmmO6+8Ek9yZ5xnoUCAAcQG8GYFs7doExax197cMY\nk6q6NMml88tvV9XnF/j6HNwJSb621UUc4czh+jCPyzOHy/uBrS5gE+nN4/K7vDxzuD7M4/LM4fIO\nuzcvElT3JTl5xeuTktx5kDH7qurYJE9L8o3VO+ruq5JclSRVtae7dx1O0UzM4fLM4fowj8szh8ur\nqj1bXcMm0psHZQ6XZw7Xh3lcnjlc3jK9eZGlvzcmOb2qTquq45JcmGT3qjG7k7xifv6yJB/v7gOO\n2gIA60JvBmBbO+QZ1e5+sKouS3JdkmOSvKu7b66qK5Ls6e7dSd6Z5H1VtTfT0doLN7JoADia6c0A\nbHeLLP1Nd1+b5NpV296w4vm3kvzM4/zaVz3O8RzIHC7PHK4P87g8c7i8o2oO9eZhmcPlmcP1YR6X\nZw6Xd9hzWFYBAQAAMJJFrlEFAACATbPhQbWqzqmqW6tqb1Vdvsb7x1fVB+b3P1VVp250TUeaBebw\nNVV1S1XdVFUfq6pnb0WdIzvUHK4Y97Kq6qpyh7dVFpnDqvrZ+Wfx5qr6zc2ucXQL/C6fUlXXV9Vn\n5t/n87aizpFV1buq6q6DfYRKTf7xPMc3VdULNrvGI4HevDy9eXl68/L05uXpzcvbsN7c3Rv2yHSD\nhy8meU6S45J8LsnOVWN+Icnb5+cXJvnARtZ0pD0WnMMXJ3nS/PzV5vDxz+E87qlJPpHkhiS7trru\nkR4L/hyenuQzSb53fv3Mra57pMeCc3hVklfPz3cmuX2r6x7tkeSnkrwgyecP8v55ST6a6TNEz0ry\nqa2uebSH3rxpc6g3LzmH8zi9eYk51JvXZQ715kPP44b05o0+o3pmkr3dfVt335/k6iQXrBpzQZL3\nzM8/lOQlVbXWh5QfrQ45h919fXffN7+8IdPn6bHfIj+HSfLmJG9J8q3NLO4Iscgc/nySK7v7niTp\n7rs2ucbRLTKHneR75udPy4Gfi3nU6+5PZI3PAl3hgiTv7ckNSZ5eVc/anOqOGHrz8vTm5enNy9Ob\nl6c3r4ON6s0bHVRPTHLHitf75m1rjunuB5Pcm+QZG1zXkWSROVzpkkxHLNjvkHNYVc9PcnJ3f2Qz\nCzuCLPJz+Nwkz62qP6iqG6rqnE2r7siwyBy+KcnFVbUv091cf2lzSttWHu+/mUcjvXl5evPy9Obl\n6c3L05s3x2H15oU+nmYJax19XX2b4UXGHM0Wnp+qujjJriQv3NCKjjyPOYdV9YQkb0vyys0q6Ai0\nyM/hsZmWGL0o05mD36uqM7r7mxtc25FikTm8KMm7u/tXq+rHM30G5hnd/fDGl7dt6CmHpjcvT29e\nnt68PL15eXrz5jisnrLRZ1T3JTl5xeuTcuDp8u+MqapjM51Sf6xTx0ebReYwVXV2ktcnOb+7v71J\ntR0pDjWHT01yRpLfrarbM62d3+2mDY+y6O/yh7v7ge7+UpJbMzVHJovM4SVJrkmS7v5kku9KcsKm\nVLd9LPRv5lFOb16e3rw8vXl5evPy9ObNcVi9eaOD6o1JTq+q06rquEw3ZNi9aszuJK+Yn78sycd7\nvuqWJAvM4bw05h2ZGqFrDw70mHPY3fd29wndfWp3n5rpWqLzu3vP1pQ7pEV+l387081DUlUnZFpu\ndNumVjm2Rebwy0lekiRV9YOZmuHdm1rlkW93kpfPdxg8K8m93f2VrS5qMHrz8vTm5enNy9Obl6c3\nb47D6s0buvS3ux+sqsuSXJfprlrv6u6bq+qKJHu6e3eSd2Y6hb4309HaCzeypiPNgnP41iRPSfLB\n+V4XX+7u87es6MEsOIc8hgXn8Lokf7WqbknyUJLXdffXt67qsSw4h69N8htV9XcyLYl5pXDwaFX1\n/kxL2E6Yrxd6Y5InJkl3vz3T9UPnJdmb5L4kr9qaSselNy9Pb16e3rw8vXl5evP62KjeXOYZAACA\nkWz00l8AAAB4XARVAAAAhiKoAgAAMBRBFQAAgKEIqgAAAAxFUAUAAGAogioAAABDEVQBAAAYyv8P\nxV9QejoNsWcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14111780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jul 11 17:35:20 2018\n",
    "\n",
    "@author: hcc\n",
    "\"\"\"\n",
    "\n",
    "#coding:utf-8\n",
    "import h5py\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "font = fm.FontProperties(fname='C:\\Windows\\Fonts\\STHUPO.TTF')\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers_strcuture, print_cost = False):\n",
    "        self.layers_strcuture = layers_strcuture\n",
    "        self.layers_num = len(layers_strcuture)\n",
    "        # 除掉输入层的网络层数，因为其他层才是真正的神经元层\n",
    "        self.param_layers_num = self.layers_num - 1\n",
    "        self.learning_rate = 0.0618\n",
    "        self.num_iterations = 2000\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.w = dict()\n",
    "        self.b = dict()\n",
    "        self.costs = []\n",
    "        self.print_cost = print_cost\n",
    "        self.init_w_and_b()\n",
    "    def set_learning_rate(self, learning_rate):\n",
    "        \"\"\"设置学习率\"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "    def set_num_iterations(self, num_iterations):\n",
    "        \"\"\"设置迭代次数\"\"\"\n",
    "        self.num_iterations = num_iterations\n",
    "    def set_xy(self, input, expected_output):\n",
    "        \"\"\"设置神经网络的输入和期望的输出\"\"\"\n",
    "        self.x = input\n",
    "        self.y = expected_output\n",
    "    def init_w_and_b(self):\n",
    "        \"\"\"\n",
    "        函数:\n",
    "            初始化神经网络所有参数\n",
    "        输入:\n",
    "            layers_strcuture: 神经网络的结构，例如[2,4,3,1]，4层结构:\n",
    "                第0层输入层接收2个数据，第1层隐藏层4个神经元，第2层隐藏层3个神经元，第3层输出层1个神经元\n",
    "        返回: 神经网络各层参数的索引表，用来定位权值 wᵢ  和偏置 bᵢ，i为网络层编号\n",
    "        \"\"\"\n",
    "        np.random.seed(3)\n",
    "        # 当前神经元层的权值为 n_i x n_(i-1)的矩阵，i为网络层编号，n为下标i代表的网络层的节点个数\n",
    "        # 例如[2,4,3,1]，4层结构：第0层输入层为2，那么第1层隐藏层神经元个数为4\n",
    "        # 那么第1层的权值w是一个 4x2 的矩阵，如：\n",
    "        #    w1 = array([ [-0.96927756, -0.59273074],\n",
    "        #                 [ 0.58227367,  0.45993021],\n",
    "        #                 [-0.02270222,  0.13577601],\n",
    "        #                 [-0.07912066, -1.49802751] ])\n",
    "        # 当前层的偏置一般给0就行，偏置是个1xnᵢ的矩阵，nᵢ为第i层的节点个数，例如第1层为4个节点，那么：\n",
    "        #    b1 = array([ 0.,  0.,  0.,  0.])\n",
    "        for l in range(1, self.layers_num):\n",
    "            self.w[\"w\" + str(l)] = np.random.randn(self.layers_strcuture[l], self.layers_strcuture[l-1])/np.sqrt(self.layers_strcuture[l-1])\n",
    "            self.b[\"b\" + str(l)] = np.zeros((self.layers_strcuture[l], 1))\n",
    "        return self.w, self.b\n",
    "    def layer_activation_forward(self, x, w, b, activation_choose):\n",
    "        \"\"\"\n",
    "        函数：\n",
    "            网络层的正向传播\n",
    "        输入：\n",
    "            x: 当前网络层输入（即上一层的输出），一般是所有训练数据，即输入矩阵\n",
    "            w: 当前网络层的权值矩阵\n",
    "            b: 当前网络层的偏置矩阵\n",
    "            activation_choose: 选择激活函数 \"sigmoid\", \"relu\", \"tanh\"\n",
    "        返回:\n",
    "            output: 网络层的激活输出\n",
    "            cache: 缓存该网络层的信息，供后续使用： (x, w, b, input_sum) -> cache\n",
    "     \"\"\"\n",
    "        # 对输入求加权和，见式（3.1）\n",
    "        input_sum = np.dot(w, x) + b\n",
    "        # 对输入加权和进行激活输出\n",
    "        # \",_\"表示output被赋值后不是元组类型\n",
    "        output, _ = activated(activation_choose, input_sum)\n",
    "        return output, (x, w, b, input_sum)\n",
    "    def forward_propagation(self, x):\n",
    "        \"\"\"\n",
    "        函数:\n",
    "            神经网络的正向传播\n",
    "        输入:\n",
    "        返回:\n",
    "            output: 正向传播完成后的输出层的输出\n",
    "            caches: 正向传播过程中缓存每一个网络层的信息： (x, w, b, input_sum),... -> caches\n",
    "        \"\"\"\n",
    "        caches = []\n",
    "        #作为输入层，输出 = 输入\n",
    "        output_prev = x\n",
    "        #第0层为输入层，只负责观察到输入的数据，并不需要处理，正向传播从第1层开始，一直到输出层输出为止\n",
    "        # range(1, n) => [1, 2, ..., n-1]\n",
    "        L = self.param_layers_num\n",
    "        for l in range(1, L):\n",
    "            # 当前网络层的输入来自前一层的输出\n",
    "            input_cur = output_prev\n",
    "            output_prev, cache = self.layer_activation_forward(input_cur, self.w[\"w\"+ str(l)], self.b[\"b\" + str(l)], \"tanh\")\n",
    "            caches.append(cache)\n",
    "        output, cache = self.layer_activation_forward(output_prev, self.w[\"w\" + str(L)], self.b[\"b\" + str(L)], \"sigmoid\")\n",
    "        caches.append(cache)\n",
    "        return output, caches\n",
    "    def show_caches(self, caches):\n",
    "        \"\"\"显示网络层的缓存参数信息\"\"\"\n",
    "        i = 1\n",
    "        for cache in caches:\n",
    "            print(\"%dtd Layer\" % i)\n",
    "            print(\" input: %s\" % cache[0])\n",
    "            print(\" w: %s\" % cache[1])\n",
    "            print(\" b: %s\" % cache[2])\n",
    "            print(\" input_sum: %s\" % cache[3])\n",
    "            print(\"----------\")\n",
    "            i += 1\n",
    "    def compute_error(self, output):\n",
    "        \"\"\"\n",
    "        函数:\n",
    "            计算档次迭代的输出总误差\n",
    "        输入:\n",
    "        返回:\n",
    "        \"\"\"\n",
    "        m = self.y.shape[1]\n",
    "        # 计算误差，见式(3.5): E = Σ1/2(期望输出-实际输出)²\n",
    "        #error = np.sum(0.5 * (self.y - output) ** 2) / m\n",
    "        # 交叉熵作为误差函数\n",
    "        error =  -np.sum(np.multiply(np.log(output),self.y) + np.multiply(np.log(1 - output), 1 - self.y)) / m\n",
    "        error = np.squeeze(error)\n",
    "        return error\n",
    "    def layer_activation_backward(self, derror_wrt_output, cache, activation_choose):\n",
    "        \"\"\"\n",
    "            函数:\n",
    "                网络层的反向传播\n",
    "            输入:\n",
    "                derror_wrt_output: 误差关于输出的偏导\n",
    "                cache: 网络层的缓存信息 (x, w, b, input_sum)\n",
    "                activation_choose: 选择激活函数 \"sigmoid\", \"relu\", \"tanh\"\n",
    "            返回: 梯度信息，即\n",
    "                derror_wrt_output_prev: 反向传播到上一层的误差关于输出的梯度\n",
    "                derror_wrt_dw: 误差关于权值的梯度\n",
    "                derror_wrt_db: 误差关于偏置的梯度\n",
    "        \"\"\"\n",
    "        input, w, b, input_sum = cache\n",
    "        output_prev = input     # 上一层的输出 = 当前层的输入; 注意是'输入'不是输入的加权和（input_sum）\n",
    "        m = output_prev.shape[1]      # m是输入的样本数量，我们要取均值，所以下面的求值要除以m\n",
    "        # 实现式（3.13）-> 误差关于权值w的偏导数\n",
    "        print(\"\")\n",
    "        derror_wrt_dinput = activated_back_propagation(activation_choose, derror_wrt_output, input_sum)\n",
    "        derror_wrt_dw = np.dot(derror_wrt_dinput, output_prev.T) / m\n",
    "        # 实现式 （3.37）-> 误差关于偏置b的偏导数\n",
    "        derror_wrt_db = np.sum(derror_wrt_dinput, axis=1, keepdims=True)/m\n",
    "        # 为反向传播到上一层提供误差传递，见式（3.33）的 （Σδ·w） 部分\n",
    "        derror_wrt_output_prev = np.dot(w.T, derror_wrt_dinput)\n",
    "        return derror_wrt_output_prev, derror_wrt_dw, derror_wrt_db\n",
    "    def back_propagation(self, output, caches):\n",
    "        \"\"\"\n",
    "        函数:\n",
    "            神经网络的反向传播\n",
    "        输入:\n",
    "            output：神经网络输\n",
    "            caches：所有网络层（输入层不算）的缓存参数信息  [(x, w, b, input_sum), ...]\n",
    "        返回:\n",
    "            grads: 返回当前迭代的梯度信息\n",
    "        \"\"\"\n",
    "        grads = {}\n",
    "        L = self.param_layers_num #\n",
    "        output = output.reshape(output.shape)  # 把输出层输出输出重构成和期望输出一样的结构\n",
    "        expected_output = self.y\n",
    "        # 见式(3.8)\n",
    "        #derror_wrt_output = -(expected_output - output)\n",
    "        # 交叉熵作为误差函数\n",
    "        derror_wrt_output = - (np.divide(expected_output, output) - np.divide(1 - expected_output, 1 - output))\n",
    "        # 反向传播：输出层 -> 隐藏层，得到梯度：见式(3.9), (3.13), (3.16)\n",
    "        current_cache = caches[L - 1] # 取最后一层,即输出层的参数信息\n",
    "        grads[\"derror_wrt_output\" + str(L)], grads[\"derror_wrt_dw\" + str(L)], grads[\"derror_wrt_db\" + str(L)] = \\\n",
    "            self.layer_activation_backward(derror_wrt_output, current_cache, \"sigmoid\")\n",
    "        # 反向传播：隐藏层 -> 隐藏层，得到梯度：见式 (3.33)的(Σδ·w), (3.33), (3.36)\n",
    "        for l in reversed(range(L - 1)):\n",
    "            current_cache = caches[l]\n",
    "            derror_wrt_output_prev_temp, derror_wrt_dw_temp, derror_wrt_db_temp = \\\n",
    "                self.layer_activation_backward(grads[\"derror_wrt_output\" + str(l + 2)], current_cache, \"tanh\")\n",
    "            grads[\"derror_wrt_output\" + str(l + 1)] = derror_wrt_output_prev_temp\n",
    "            grads[\"derror_wrt_dw\" + str(l + 1)] = derror_wrt_dw_temp\n",
    "            grads[\"derror_wrt_db\" + str(l + 1)] = derror_wrt_db_temp\n",
    "        return grads\n",
    "    def update_w_and_b(self, grads):\n",
    "        \"\"\"\n",
    "        函数:\n",
    "            根据梯度信息更新w，b\n",
    "        输入:\n",
    "            grads：当前迭代的梯度信息\n",
    "        返回:\n",
    "        \"\"\"\n",
    "        # 权值w和偏置b的更新，见式:（3.17),(3.19)\n",
    "        for l in range(self.param_layers_num):\n",
    "            self.w[\"w\" + str(l + 1)] = self.w[\"w\" + str(l + 1)] - self.learning_rate * grads[\"derror_wrt_dw\" + str(l + 1)]\n",
    "            self.b[\"b\" + str(l + 1)] = self.b[\"b\" + str(l + 1)] - self.learning_rate * grads[\"derror_wrt_db\" + str(l + 1)]\n",
    "    def training_modle(self):\n",
    "        \"\"\"训练神经网络模型\"\"\"\n",
    "        np.random.seed(5)\n",
    "        for i in range(0, self.num_iterations):\n",
    "            # 正向传播，得到网络输出，以及每一层的参数信息\n",
    "            output, caches = self.forward_propagation(self.x)\n",
    "            # 计算网络输出误差\n",
    "            cost = self.compute_error(output)\n",
    "            # 反向传播，得到梯度信息\n",
    "            grads = self.back_propagation(output, caches)\n",
    "            # 根据梯度信息，更新权值w和偏置b\n",
    "            self.update_w_and_b(grads)\n",
    "            # 当次迭代结束，打印误差信息\n",
    "            if self.print_cost and i % 1000 == 0:\n",
    "                print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "            if self.print_cost and i % 1000 == 0:\n",
    "                self.costs.append(cost)\n",
    "        # 模型训练完后显示误差曲线\n",
    "        if False:\n",
    "            plt.plot(np.squeeze(self.costs))\n",
    "            plt.ylabel(u'神经网络误差', fontproperties = font)\n",
    "            plt.xlabel(u'迭代次数 (*100)', fontproperties = font)\n",
    "            plt.title(u\"学习率 =\" + str(self.learning_rate), fontproperties = font)\n",
    "            plt.show()\n",
    "        return self.w, self.b\n",
    "    def predict_by_modle(self, x):\n",
    "        \"\"\"使用训练好的模型（即最后求得w，b参数）来决策输入的样本的结果\"\"\"\n",
    "        output, _ = self.forward_propagation(x.T)\n",
    "        \n",
    "        output = output.T\n",
    "        result = output / np.sum(output, axis=1, keepdims=True)\n",
    "        return np.argmax(result, axis=1)\n",
    "def sigmoid(input_sum):\n",
    "    \"\"\"\n",
    "    函数：\n",
    "        激活函数Sigmoid\n",
    "    输入：\n",
    "        input_sum: 输入，即神经元的加权和\n",
    "    返回：\n",
    "        output: 激活后的输出\n",
    "        input_sum: 把输入缓存起来返回\n",
    "    \"\"\"\n",
    "    output = 1.0/(1+np.exp(-input_sum))\n",
    "    return output, input_sum\n",
    "def sigmoid_back_propagation(derror_wrt_output, input_sum):\n",
    "    \"\"\"\n",
    "    函数：\n",
    "        误差关于神经元输入的偏导: dE／dIn = dE/dOut * dOut/dIn  参照式（3.6）\n",
    "        其中： dOut/dIn 就是激活函数的导数 dy=y(1 - y)，见式（3.9）\n",
    "              dE/dOut 误差对神经元输出的偏导，见式（3.8）\n",
    "    输入：\n",
    "        derror_wrt_output：误差关于神经元输出的偏导: dE/dyⱼ = 1/2(d(expect_to_output - output)**2/doutput) = -(expect_to_output - output)\n",
    "        input_sum: 输入加权和\n",
    "    返回：\n",
    "        derror_wrt_dinputs: 误差关于输入的偏导，见式（3.13）\n",
    "    \"\"\"\n",
    "    output = 1.0/(1 + np.exp(- input_sum))\n",
    "    doutput_wrt_dinput = output * (1 - output)\n",
    "    derror_wrt_dinput =  derror_wrt_output * doutput_wrt_dinput\n",
    "    return derror_wrt_dinput\n",
    "def relu(input_sum):\n",
    "    \"\"\"\n",
    "        函数：\n",
    "            激活函数ReLU\n",
    "        输入：\n",
    "            input_sum: 输入，即神经元的加权和\n",
    "        返回：\n",
    "            outputs: 激活后的输出\n",
    "            input_sum: 把输入缓存起来返回\n",
    "    \"\"\"\n",
    "    output = np.maximum(0, input_sum)\n",
    "    return output, input_sum\n",
    "def relu_back_propagation(derror_wrt_output, input_sum):\n",
    "    \"\"\"\n",
    "        函数：\n",
    "            误差关于神经元输入的偏导: dE／dIn = dE/dOut * dOut/dIn\n",
    "            其中： dOut/dIn 就是激活函数的导数\n",
    "                  dE/dOut 误差对神经元输出的偏导\n",
    "        输入：\n",
    "            derror_wrt_output：误差关于神经元输出的偏导\n",
    "            input_sum: 输入加权和\n",
    "        返回：\n",
    "            derror_wrt_dinputs: 误差关于输入的偏导\n",
    "    \"\"\"\n",
    "    derror_wrt_dinputs = np.array(derror_wrt_output, copy=True)\n",
    "    derror_wrt_dinputs[input_sum <= 0] = 0\n",
    "    return derror_wrt_dinputs\n",
    "def tanh(input_sum):\n",
    "    \"\"\"\n",
    "    函数：\n",
    "        激活函数 tanh\n",
    "    输入：\n",
    "        input_sum: 输入，即神经元的加权和\n",
    "    返回：\n",
    "        output: 激活后的输出\n",
    "        input_sum: 把输入缓存起来返回\n",
    "    \"\"\"\n",
    "    output = np.tanh(input_sum)\n",
    "    return output, input_sum\n",
    "def tanh_back_propagation(derror_wrt_output, input_sum):\n",
    "    \"\"\"\n",
    "    函数：\n",
    "        误差关于神经元输入的偏导: dE／dIn = dE/dOut * dOut/dIn\n",
    "        其中： dOut/dIn 就是激活函数的导数 tanh'(x) = 1 - x²\n",
    "              dE/dOut 误差对神经元输出的偏导\n",
    "    输入：\n",
    "        derror_wrt_output：误差关于神经元输出的偏导: dE/dyⱼ = 1/2(d(expect_to_output - output)**2/doutput) = -(expect_to_output - output)\n",
    "        input_sum: 输入加权和\n",
    "    返回：\n",
    "        derror_wrt_dinputs: 误差关于输入的偏导\n",
    "    \"\"\"\n",
    "    output = np.tanh(input_sum)\n",
    "    doutput_wrt_dinput = 1 - np.power(output, 2)\n",
    "    derror_wrt_dinput =  derror_wrt_output * doutput_wrt_dinput\n",
    "    return derror_wrt_dinput\n",
    "def activated(activation_choose, input):\n",
    "    \"\"\"把正向激活包装一下\"\"\"\n",
    "    if activation_choose == \"sigmoid\":\n",
    "        return sigmoid(input)\n",
    "    elif activation_choose == \"relu\":\n",
    "        return relu(input)\n",
    "    elif activation_choose == \"tanh\":\n",
    "        return tanh(input)\n",
    "    return sigmoid(input)\n",
    "def activated_back_propagation(activation_choose, derror_wrt_output, output):\n",
    "    \"\"\"包装反向激活传播\"\"\"\n",
    "    if activation_choose == \"sigmoid\":\n",
    "        return sigmoid_back_propagation(derror_wrt_output, output)\n",
    "    elif activation_choose == \"relu\":\n",
    "        return relu_back_propagation(derror_wrt_output, output)\n",
    "    elif activation_choose == \"tanh\":\n",
    "        return tanh_back_propagation(derror_wrt_output, output)\n",
    "    return sigmoid_back_propagation(derror_wrt_output, output)\n",
    "def plot_decision_boundary(xy, colors, pred_func):\n",
    "    # xy是坐标点的集合，把集合的范围算出来\n",
    "    # 加减0.5相当于扩大画布的范围，不然画出来的图坐标点会落在图的边缘，逼死强迫症患者\n",
    "    x_min, x_max = xy[:, 0].min() - 0.5, xy[:, 0].max() + 0.5\n",
    "    y_min, y_max = xy[:, 1].min() - 0.5, xy[:, 1].max() + 0.5\n",
    "    # 以h为分辨率，生成采样点的网格，就像一张网覆盖所有颜色点\n",
    "    h = .01\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # 把网格点集合作为输入到模型，也就是预测这个采样点是什么颜色的点，从而得到一个决策面\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # 利用等高线，把预测的结果画出来，效果上就是画出红蓝点的分界线\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    # 训练用的红蓝点点也画出来\n",
    "    plt.scatter(xy[:, 0], xy[:, 1], c=colors, marker='o', cmap=plt.cm.Spectral, edgecolors='black')\n",
    "if __name__ == \"__main__\":\n",
    "    plt.figure(figsize=(16, 32))\n",
    "    # 用sklearn的数据样本集，产生2种颜色的坐标点，noise是噪声系数，噪声越大，2种颜色的点分布越凌乱\n",
    "    xy, colors = sklearn.datasets.make_moons(60, noise=1.0)\n",
    "     \n",
    "    # 因为点的颜色是1bit，我们设计一个神经网络，输出层有2个神经元。\n",
    "    # 标定输出[1,0]为红色点，输出[0,1]为蓝色点\n",
    "    expect_output = []\n",
    "    for c in colors:\n",
    "        if c == 1:\n",
    "            expect_output.append([0,1])\n",
    "        else:\n",
    "            expect_output.append([1,0])\n",
    "    print(np.shape(expect_output))\n",
    "    expect_output = np.array(expect_output).T\n",
    "    print(np.shape(expect_output))\n",
    "    print(\"expect_output\" ,expect_output)\n",
    "    \n",
    "    # 设计3层网络，改变隐藏层神经元的个数，观察神经网络分类红蓝点的效果\n",
    "    hidden_layer_neuron_num_list = [1,2,4,10,20,50]\n",
    "    for i, hidden_layer_neuron_num in enumerate(hidden_layer_neuron_num_list):\n",
    "        plt.subplot(5, 2, i + 1)\n",
    "        plt.title(u'隐藏层神经元数量: %d' % hidden_layer_neuron_num, fontproperties = font)\n",
    "        nn = NeuralNetwork([2, hidden_layer_neuron_num, 2], True)\n",
    "        # 输出和输入层都是2个节点，所以输入和输出的数据集合都要是 nx2的矩阵\n",
    "        nn.set_xy(xy.T, expect_output)\n",
    "        nn.set_num_iterations(30000)\n",
    "        nn.set_learning_rate(0.1)\n",
    "        w, b = nn.training_modle()\n",
    "        plot_decision_boundary(xy, colors, nn.predict_by_modle)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://zhuanlan.zhihu.com/p/38006693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jul 11 17:35:20 2018\n",
    "\n",
    "@author: hcc\n",
    "\"\"\"\n",
    "\n",
    "#coding:utf-8\n",
    "import h5py\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "font = fm.FontProperties(fname='C:\\Windows\\Fonts\\STXIHEI.TTF')\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "def sigmoid(input_sum):\n",
    "    output = 1.0/(1+np.exp(-input_sum))\n",
    "    return output, input_sum\n",
    "def sigmoid_back_propagation(derror_wrt_output, input_sum):\n",
    "    \"\"\"\n",
    "    函数：\n",
    "        误差关于神经元输入的偏导: dE／dIn = dE/dOut * dOut/dIn  参照式（3.6）\n",
    "        其中： dOut/dIn 就是激活函数的导数 dy=y(1 - y)，见式（3.9）\n",
    "              dE/dOut 误差对神经元输出的偏导，见式（3.8）\n",
    "    输入：\n",
    "        derror_wrt_output：误差关于神经元输出的偏导: dE/dyⱼ = 1/2(d(expect_to_output - output)**2/doutput) = -(expect_to_output - output)\n",
    "        input_sum: 输入加权和\n",
    "    返回：\n",
    "        derror_wrt_dinputs: 误差关于输入的偏导，见式（3.13）\n",
    "    \"\"\"\n",
    "    output = 1.0/(1 + np.exp(- input_sum))\n",
    "    doutput_wrt_dinput = output * (1 - output)\n",
    "    derror_wrt_dinput =  derror_wrt_output * doutput_wrt_dinput\n",
    "    return derror_wrt_dinput\n",
    "def relu(input_sum):\n",
    "    output = np.maximum(0, input_sum)\n",
    "    return output, input_sum\n",
    "def relu_back_propagation(derror_wrt_output, input_sum):\n",
    "    derror_wrt_dinputs = np.array(derror_wrt_output, copy=True)\n",
    "    derror_wrt_dinputs[input_sum <= 0] = 0\n",
    "    return derror_wrt_dinputs\n",
    "def tanh(input_sum):\n",
    "    output = np.tanh(input_sum)\n",
    "    return output, input_sum\n",
    "def tanh_back_propagation(derror_wrt_output, input_sum):\n",
    "    \n",
    "    output = np.tanh(input_sum)\n",
    "    doutput_wrt_dinput = 1 - np.power(output, 2)\n",
    "    derror_wrt_dinput =  derror_wrt_output * doutput_wrt_dinput\n",
    "    return derror_wrt_dinput\n",
    "def activated(activation_choose, input):\n",
    "    \"\"\"把正向激活包装一下\"\"\"\n",
    "    if activation_choose == \"sigmoid\":\n",
    "        return sigmoid(input)\n",
    "    elif activation_choose == \"relu\":\n",
    "        return relu(input)\n",
    "    elif activation_choose == \"tanh\":\n",
    "        return tanh(input)\n",
    "    return sigmoid(input)\n",
    "def activated_back_propagation(activation_choose, derror_wrt_output, output):\n",
    "    \"\"\"包装反向激活传播\"\"\"\n",
    "    if activation_choose == \"sigmoid\":\n",
    "        return sigmoid_back_propagation(derror_wrt_output, output)\n",
    "    elif activation_choose == \"relu\":\n",
    "        return relu_back_propagation(derror_wrt_output, output)\n",
    "    elif activation_choose == \"tanh\":\n",
    "        return tanh_back_propagation(derror_wrt_output, output)\n",
    "    return sigmoid_back_propagation(derror_wrt_output, output)\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers_strcuture, print_cost = False):\n",
    "        self.layers_strcuture = layers_strcuture\n",
    "        self.layers_num = len(layers_strcuture)\n",
    "        # 除掉输入层的网络层数，因为其他层才是真正的神经元层\n",
    "        self.param_layers_num = self.layers_num - 1\n",
    "        self.learning_rate = 0.0618\n",
    "        self.num_iterations = 2000\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.w = dict()\n",
    "        self.b = dict()\n",
    "        self.costs = []\n",
    "        self.print_cost = print_cost\n",
    "        self.init_w_and_b()\n",
    "    def set_learning_rate(self, learning_rate):\n",
    "        \"\"\"设置学习率\"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "    def set_num_iterations(self, num_iterations):\n",
    "        \"\"\"设置迭代次数\"\"\"\n",
    "        self.num_iterations = num_iterations\n",
    "    def set_xy(self, input, expected_output):\n",
    "        \"\"\"设置神经网络的输入和期望的输出\"\"\"\n",
    "        self.x = input\n",
    "        self.y = expected_output\n",
    "        print(\"init x,y-------------set_xy--------------:\\n\")\n",
    "        print(\"x:\\n\",self.x)\n",
    "        print(\"y:\\n\",self.y)\n",
    "    def init_w_and_b(self):\n",
    "        print(\"--------------------------init_w_and_b----------------------------------\\n\")\n",
    "        print(\"layers_num\",self.layers_num)\n",
    "        print(\"layers_strcuture:\",self.layers_strcuture)\n",
    "        np.random.seed(3)\n",
    "        for l in range(1, self.layers_num):\n",
    "           \n",
    "            self.w[\"w\" + str(l)] = np.random.randn(self.layers_strcuture[l], self.layers_strcuture[l-1])/np.sqrt(self.layers_strcuture[l-1]) \n",
    "            self.b[\"b\" + str(l)] = np.zeros((self.layers_strcuture[l], 1))\n",
    "           \n",
    "        print(\"after init w,b----------------:\\n\")\n",
    "        print(\"w:\\n\",self.w)\n",
    "        print(\"b:\\n\" ,self.b)\n",
    "        print(\"------------------------------------------------------------\\n\")\n",
    "        return self.w, self.b\n",
    "    def layer_activation_forward(self, x, w, b, activation_choose):\n",
    "        print(\"---------------------------layer_activation_forward---------------------------------\\n\")\n",
    "        print(\"layer_activation_forward :\\n\")\n",
    "        # 对输入求加权和，见式（5.1）\n",
    "        input_sum = np.dot(w, x) + b\n",
    "        \n",
    "        print(\"shape of W:\",np.shape(w))\n",
    "        print(\"shape of X:\",np.shape(x))\n",
    "        print(\"w:\\n\",w)\n",
    "        print(\"x:\\n\",x)\n",
    "        print(\"b:\\n\",b)\n",
    "        print(\"input_sum:\\n\",input_sum)\n",
    "        # 对输入加权和进行激活输出\n",
    "        output, _ = activated(activation_choose, input_sum)   \n",
    "        print(\"------------------------------------------------------------\\n\")\n",
    "        return output, (x, w, b, input_sum)\n",
    "    def forward_propagation(self, x):\n",
    "        print(\"-----------------------------forward_propagation-------------------------------\\n\")\n",
    "        caches = []\n",
    "        #作为输入层，输出 = 输入\n",
    "        output_prev = x\n",
    "        #第0层为输入层，只负责观察到输入的数据，并不需要处理，正向传播从第1层开始，一直到输出层输出为止\n",
    "        # range(1, n) => [1, 2, ..., n-1]\n",
    "        L = self.param_layers_num\n",
    "        for l in range(1, L):\n",
    "            # 当前网络层的输入来自前一层的输出\n",
    "            input_cur = output_prev\n",
    "            output_prev, cache = self.layer_activation_forward(input_cur, self.w[\"w\"+ str(l)], self.b[\"b\" + str(l)], \"tanh\")\n",
    "            print(\"output_prev:\\n\",output_prev)\n",
    "            print(\"cache:\\n\",cache)\n",
    "            caches.append(cache)\n",
    "        output, cache = self.layer_activation_forward(output_prev, self.w[\"w\" + str(L)], self.b[\"b\" + str(L)], \"sigmoid\")\n",
    "        caches.append(cache)\n",
    "        print(\"forward_propagation cache:\\n\",cache)\n",
    "        print(\"output:\\n\",output)\n",
    "        print(\"------------------------------------------------------------\\n\")\n",
    "        return output, caches\n",
    "    def show_caches(self, caches):\n",
    "        print(\"-----------------------------show_caches-------------------------------\\n\")\n",
    "        \"\"\"显示网络层的缓存参数信息\"\"\"\n",
    "        i = 1\n",
    "        for cache in caches:\n",
    "            print(\"%dtd Layer\" % i)\n",
    "            print(\" input: %s\" % cache[0])\n",
    "            print(\" w: %s\" % cache[1])\n",
    "            print(\" b: %s\" % cache[2])\n",
    "            print(\" input_sum: %s\" % cache[3])\n",
    "            print(\"---------------\")\n",
    "            i += 1\n",
    "        print(\"------------------------------------------------------------\\n\")\n",
    "    def compute_error(self, output):\n",
    "        print(\"---------------------------compute_error---------------------------------\\n\")\n",
    "        m = self.y.shape[1]\n",
    "         # 计算误差，见式(3.5): E = Σ1/2(期望输出-实际输出)²\n",
    "        #error = np.sum(0.5 * (self.y - output) ** 2) / m\n",
    "        # 交叉熵作为误差函数\n",
    "        error =  -np.sum(np.multiply(np.log(output),self.y) + np.multiply(np.log(1 - output), 1 - self.y)) / m\n",
    "        print(\"error-befoer:\\n\",error)\n",
    "        error = np.squeeze(error)\n",
    "        print(\"error:\\n\",error)\n",
    "        print(\"------------------------------------------------------------\\n\")\n",
    "        return error\n",
    "    def layer_activation_backward(self, derror_wrt_output, cache, activation_choose):\n",
    " \n",
    "        print(\"---------------------------layer_activation_backward---------------------------------\\n\")\n",
    "        input, w, b, input_sum = cache\n",
    "        output_prev = input     # 上一层的输出 = 当前层的输入; 注意是'输入'不是输入的加权和（input_sum）\n",
    "        m = output_prev.shape[1]      # m是输入的样本数量，我们要取均值，所以下面的求值要除以m\n",
    "        # 实现式（3.13）-> 误差关于权值w的偏导数\n",
    "        \"\"\"\n",
    "        #derror_wrt_output = -(expected_output - output)\n",
    "        # 交叉熵作为误差函数\n",
    "        #derror_wrt_output = - (np.divide(expected_output, output) - np.divide(1 - expected_output, 1 - output))\n",
    "         output = 1.0/(1 + np.exp(- input_sum))\n",
    "         doutput_wrt_dinput = output * (1 - output)\n",
    "         derror_wrt_dinput =  derror_wrt_output * doutput_wrt_dinput\n",
    "         derror_wrt_dinput = (Cost)output(1-output)  //Cost = derror_wrt_output\n",
    "          \n",
    "        \"\"\"\n",
    "        derror_wrt_dinput = activated_back_propagation(activation_choose, derror_wrt_output, input_sum)\n",
    "       \n",
    "        print(\"shape of derror_wrt_dinput:\\n\", np.shape(derror_wrt_dinput))\n",
    "        print(\"derror_wrt_dinput:\\n\",derror_wrt_dinput)\n",
    "        print(\"shape of output_prev:\\n\",np.shape(output_prev.T))\n",
    "       \n",
    "        derror_wrt_dw = np.dot(derror_wrt_dinput, output_prev.T) / m\n",
    "        \n",
    "        print(\"derror_wrt_dw:\\n\",derror_wrt_dw)\n",
    "        \n",
    "          # 实现式 （3.37）-> 误差关于偏置b的偏导数\n",
    "        derror_wrt_db = np.sum(derror_wrt_dinput, axis=1, keepdims=True)/m  #axis = 1按列，keepims = True，压缩，即按列压缩，由2行8列压缩为2行1列再取平均\n",
    "        print(\"shape of derror_wrt_db\",np.shape(derror_wrt_db))\n",
    "        # 为反向传播到上一层提供误差传递，见式（3.33）的 （Σδ·w） 部分\n",
    "        print(\"shape of w:\",np.shape(w))\n",
    "        print(\"shape of derror_wrt_dinput:\",np.shape(derror_wrt_dinput))\n",
    "        derror_wrt_output_prev = np.dot(w.T, derror_wrt_dinput)\n",
    "        \n",
    "        print(\"w T:\\n\", w.T)\n",
    "        print(\"derror_wrt_output_prev:\\n\",derror_wrt_output_prev)\n",
    "        print(\"------------------------------------------------------------\\n\")\n",
    "        return derror_wrt_output_prev, derror_wrt_dw, derror_wrt_db\n",
    "    def back_propagation(self, output, caches):\n",
    "        \"\"\"\n",
    "        函数:\n",
    "            神经网络的反向传播\n",
    "        输入:\n",
    "            output：神经网络输\n",
    "            caches：所有网络层（输入层不算）的缓存参数信息  [(x, w, b, input_sum), ...]\n",
    "        返回:\n",
    "            grads: 返回当前迭代的梯度信息\n",
    "        \"\"\"\n",
    "        print(\"------------------------------back_propagation------------------------------\\n\")\n",
    "        grads = {}\n",
    "        L = self.param_layers_num #\n",
    "        print(\"Layers_num:\",L)\n",
    "        print(\"shape of output:\",np.shape(output))\n",
    "        \n",
    "        output = output.reshape(output.shape)  # 把输出层输出输出重构成和期望输出一样的结构\n",
    "        print(\"after shape of output:\",np.shape(output))\n",
    "        print(\"output:\\n\",output)\n",
    "        expected_output = self.y\n",
    "        print(\"expected_output:\\n:\",expected_output)\n",
    "         # 见式(3.8)\n",
    "        #derror_wrt_output = -(expected_output - output)\n",
    "        # 交叉熵作为误差函数\n",
    "        derror_wrt_output = - (np.divide(expected_output, output) - np.divide(1 - expected_output, 1 - output))\n",
    "        print(\"derror_wrt_output:\\n\",derror_wrt_output)\n",
    "        # 反向传播：输出层 -> 隐藏层，得到梯度：见式(3.9), (3.13), (3.16)\n",
    "        current_cache = caches[L - 1] # 取最后一层,即输出层的参数信息\n",
    "        grads[\"derror_wrt_output\" + str(L)], grads[\"derror_wrt_dw\" + str(L)], grads[\"derror_wrt_db\" + str(L)] = \\\n",
    "            self.layer_activation_backward(derror_wrt_output, current_cache, \"sigmoid\")\n",
    "        print(\"grads------------------:\\n\",grads)\n",
    "        # 反向传播：隐藏层 -> 隐藏层，得到梯度：见式 (3.33)的(Σδ·w), (3.33), (3.36)\n",
    "        for l in reversed(range(L - 1)):\n",
    "            print(\"l is:\",l)\n",
    "            current_cache = caches[l]\n",
    "            derror_wrt_output_prev_temp, derror_wrt_dw_temp, derror_wrt_db_temp = \\\n",
    "                self.layer_activation_backward(grads[\"derror_wrt_output\" + str(l + 2)], current_cache, \"tanh\")\n",
    "            grads[\"derror_wrt_output\" + str(l + 1)] = derror_wrt_output_prev_temp\n",
    "            grads[\"derror_wrt_dw\" + str(l + 1)] = derror_wrt_dw_temp\n",
    "            grads[\"derror_wrt_db\" + str(l + 1)] = derror_wrt_db_temp\n",
    "        print(\"after back_propagation grads:------------\\n\",grads)\n",
    "        print(\"------------------------------------------------------------\\n\")\n",
    "        return grads\n",
    "    def update_w_and_b(self, grads):\n",
    "        \"\"\"\n",
    "        函数:\n",
    "            根据梯度信息更新w，b\n",
    "        输入:\n",
    "            grads：当前迭代的梯度信息\n",
    "        返回:\n",
    "        \"\"\"\n",
    "        print(\"-----------------------------update_w_and_b-------------------------------\\n\")\n",
    "        # 权值w和偏置b的更新，见式:（3.17),(3.19)\n",
    "        for l in range(self.param_layers_num):\n",
    "            self.w[\"w\" + str(l + 1)] = self.w[\"w\" + str(l + 1)] - self.learning_rate * grads[\"derror_wrt_dw\" + str(l + 1)]\n",
    "            self.b[\"b\" + str(l + 1)] = self.b[\"b\" + str(l + 1)] - self.learning_rate * grads[\"derror_wrt_db\" + str(l + 1)]\n",
    "        print(\"update_w_b:\\n\")\n",
    "        print(\"w:\\n\",self.w)\n",
    "        print(\"b:\\n\",self.b)\n",
    "        print(\"------------------------------------------------------------\\n\")\n",
    "    def training_modle(self):\n",
    "        \"\"\"训练神经网络模型\"\"\"\n",
    "        np.random.seed(5)\n",
    "        for i in range(0, self.num_iterations):\n",
    "            # 正向传播，得到网络输出，以及每一层的参数信息\n",
    "            #caches = (x, w, b, input_sum)\n",
    "            print(\"------------------------------training_modle------------------------------\\n\")\n",
    "            print(\"begin _training:\")\n",
    "            print(\"x:\\n\",self.x)\n",
    "            output, caches = self.forward_propagation(self.x)\n",
    "            print(\"i:%d ,output:%s\\n\"%(i,output))\n",
    "            print(\"caches:\\n\",caches)\n",
    "            # 计算网络输出误差\n",
    "            cost = self.compute_error(output)\n",
    "            # 反向传播，得到梯度信息\n",
    "            grads = self.back_propagation(output, caches)\n",
    "            # 根据梯度信息，更新权值w和偏置b\n",
    "            self.update_w_and_b(grads)\n",
    "            # 当次迭代结束，打印误差信息\n",
    "            if self.print_cost and i % 1000 == 0:\n",
    "                print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "            if self.print_cost and i % 1000 == 0:\n",
    "                self.costs.append(cost)\n",
    "        # 模型训练完后显示误差曲线\n",
    "        if False:\n",
    "            plt.plot(np.squeeze(self.costs))\n",
    "            plt.ylabel(u'神经网络误差', fontproperties = font)\n",
    "            plt.xlabel(u'迭代次数 (*100)', fontproperties = font)\n",
    "            plt.title(u\"学习率 =\" + str(self.learning_rate), fontproperties = font)\n",
    "            plt.show()\n",
    "        print(\"------------------------------------------------------------\\n\")\n",
    "        return self.w, self.b\n",
    "    def predict_by_modle(self, x):\n",
    "        \"\"\"使用训练好的模型（即最后求得w，b参数）来决策输入的样本的结果\"\"\"\n",
    "        output, _ = self.forward_propagation(x.T)\n",
    "        output = output.T\n",
    "        result = output / np.sum(output, axis=1, keepdims=True)\n",
    "        return np.argmax(result, axis=1)\n",
    "def plot_decision_boundary(xy, colors, pred_func):\n",
    "    # xy是坐标点的集合，把集合的范围算出来\n",
    "    # 加减0.5相当于扩大画布的范围，不然画出来的图坐标点会落在图的边缘，逼死强迫症患者\n",
    "    x_min, x_max = xy[:, 0].min() - 0.5, xy[:, 0].max() + 0.5\n",
    "    y_min, y_max = xy[:, 1].min() - 0.5, xy[:, 1].max() + 0.5\n",
    "    # 以h为分辨率，生成采样点的网格，就像一张网覆盖所有颜色点\n",
    "    h = .01\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # 把网格点集合作为输入到模型，也就是预测这个采样点是什么颜色的点，从而得到一个决策面\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # 利用等高线，把预测的结果画出来，效果上就是画出红蓝点的分界线\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    # 训练用的红蓝点点也画出来\n",
    "    plt.scatter(xy[:, 0], xy[:, 1], c=colors, marker='o', cmap=plt.cm.Spectral, edgecolors='black')\n",
    "if __name__ == \"__main__\":\n",
    "    plt.figure(figsize=(16, 32))\n",
    "    print(\"------------------------------------------------------------\\n\")\n",
    "    # 用sklearn的数据样本集，产生2种颜色的坐标点，noise是噪声系数，噪声越大，2种颜色的点分布越凌乱\n",
    "    xy, colors = sklearn.datasets.make_moons(8, noise=1.0)\n",
    "    print(\"xy:\\n\",xy.T)\n",
    "    print(\"colors:\\n\",colors)\n",
    "    # 因为点的颜色是1bit，我们设计一个神经网络，输出层有2个神经元。\n",
    "    # 标定输出[1,0]为红色点，输出[0,1]为蓝色点\n",
    "    expect_output = []\n",
    "    for c in colors:\n",
    "        if c == 1:\n",
    "            expect_output.append([0,1])\n",
    "        else:\n",
    "            expect_output.append([1,0])\n",
    "    expect_output = np.array(expect_output).T\n",
    "    print(\"expect_output:\\n\",expect_output)\n",
    "    # 设计3层网络，改变隐藏层神经元的个数，观察神经网络分类红蓝点的效果\n",
    "    hidden_layer_neuron_num_list = [4]\n",
    "    for i, hidden_layer_neuron_num in enumerate(hidden_layer_neuron_num_list):\n",
    "        plt.subplot(5, 2, i + 1)\n",
    "        plt.title(u'隐藏层神经元数量: %d' % hidden_layer_neuron_num, fontproperties = font)\n",
    "        nn = NeuralNetwork([2, hidden_layer_neuron_num, 2], True)\n",
    "        # 输出和输入层都是2个节点，所以输入和输出的数据集合都要是 nx2的矩阵\n",
    "        nn.set_xy(xy.T, expect_output)\n",
    "        nn.set_num_iterations(1)\n",
    "        nn.set_learning_rate(0.1)\n",
    "        w, b = nn.training_modle()\n",
    "        plot_decision_boundary(xy, colors, nn.predict_by_modle)\n",
    "    print(\"------------------------------------------------------------\\n\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
