{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "softmax(柔性最大值)函数，一般在神经网络中， softmax可以作为分类任务的输出层。其实可以认为softmax输出的是几个类别选择的概率，比如我有一个分类任务，要分为三个类，softmax函数可以根据它们相对的大小，输出三个类别选取的概率，并且概率和为1,因此\n",
    "Softmax通常被用作输出层的激活函数，这不仅是因为它的效果好，而且因为它使得连续数值转换为相对概率，理解上变得直观。同时，softmaxt配合自然对数（$ln = log_{e}$）损失\n",
    "函数其训练效果也要比采用二次代价函数的方式好。之前对一直对softmax似懂非懂，参考别人的手动推导一遍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax函数及求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax函数公式如下：\n",
    "\n",
    "$$\n",
    "    a_{j}^{L} = \\frac{e^{\\sigma_{j}^L}}{\\sum_{k}{e^{\\sigma_{k}^{L}}}}\n",
    "$$\n",
    "\n",
    "其中:\n",
    "- ${\\sigma_{j}^{L}}$表示第$L$层（通常是最后一层）第$j$个神经元的输入\n",
    "- $a_{j}^{L}$表示第$L$层softmax的第$j$个输出，\n",
    "- e表示自然常熟（exp），\n",
    "- $\\sum_{k}{e^{\\sigma_{k}^{L}}}$表示了第$L$层所有神经元的输入之和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax函数最明显的特点在于：它把每个神经元的输入占当前所有神经元之和得比值，当作神经元的输出，这使得输出更容易被解释：\n",
    "神经元的输出值越大，则该神经元对应的类别是真实类别的可能性更高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "另外，softmax不仅把神经元输出构造成概率分布，而且还起到了归一化的作用，适用于很多需要进行归一化处理的分类问题。\n",
    "由于softmax在人工神经网络（ANN）算法中的求导结果比较特别，分为两种情况，希望能帮助到正在学习此类算法的朋友们。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求导过程：\n",
    "\n",
    "$\n",
    "if\\space j = i:\n",
    "$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial{a_{j}}}{\\partial{\\sigma_{i}}} =&\\frac{\\partial}{\\partial{\\sigma_{i}}}\\left(\\frac{e^{\\sigma_{j}}}{\\sum_{k}e^{{\\sigma_{k}}}}\\right)\\\\\n",
    "=&\\frac{(e^{\\sigma_{j}})'\\sum_{k}{e^{\\sigma_{k}} - e^{\\sigma_{j}}}\\cdot e^{\\sigma_j} }{\\left(\\sum_{k}{e^{\\sigma_{k}}}\\right)^2}\\\\\n",
    "=&\\frac{e^{\\sigma_{j}}}{\\sum_{k}e^{{\\sigma_{k}}}} - \\frac{e^{\\sigma_{j}}}{\\sum_{k}e^{{\\sigma_{k}}}}\\cdot\\frac{e^{\\sigma_{j}}}{\\sum_{k}e^{{\\sigma_{k}}}}\\\\\n",
    "=&a_{j}(1-a_{j})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\n",
    "if \\space j = i:\n",
    "$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial{a_{j}}}{\\partial{\\sigma_{i}}} =&\\frac{\\partial}{\\partial{\\sigma_{i}}}\\left(\\frac{e^{\\sigma_{j}}}{\\sum_{k}e^{{\\sigma_{k}}}}\\right)\\\\\n",
    "=&0\\cdot\\sum_{k}{e^{\\sigma_{k}} - e^{\\sigma_{j}}}\\cdot e^{\\sigma_j}{\\left(\\sum_{k}{e^{\\sigma_{k}}}\\right)^2}\\\\\n",
    "=&-\\frac{e^{\\sigma_{j}}}{\\sum_{k}e^{{\\sigma_{k}}}}\\cdot\\frac{e^{\\sigma_{i}}}{\\sum_{k}e^{{\\sigma_{k}}}}\\\\\n",
    "=& -a_{j}a_{j}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax配合对数似然损失函数训练ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二次代价函数在训练ANN时可能会导致训练训练变慢的问题，初始的输出离真实值越远，训练速度就越慢，这个问题可以采用交叉熵代价函数来解决。其实这个问题也可以采用另一种方法解决，那就是\n",
    "采用softmax激活函数，并采用$\\ln$似然代价函数来解决。\n",
    "$\\ln$似然代价函数的公式为：\n",
    "\n",
    "$$\n",
    "C = -\\sum_{k}{y_{k}\\ln {a_{k}}}\n",
    "$$\n",
    "\n",
    "其中，$a_{k}$表示第k个神经元的输出值，$y_{k}$表示第$k$个神经元对应的真实值，取值为0或1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个损失函数的含义：在ANN中输入一个样本，那么只有一个神经元对应了该样本的正确类别；\n",
    "若这个神经元输出的概率值越高，则按照以上的代价函数公式，其产生的代价就越小；反之，则产生的代价就越高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了检验softmax和这个代价函数也可以解决上述所说的训练速度变慢问题，接下来的重点就是推导ANN\n",
    "的权重w和偏置b的梯度公式，以偏置b为例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial C}{\\partial b_{j}} =& \\frac{\\partial C}{\\partial \\sigma_{j}}\\cdot \\frac{\\partial \\sigma_{j}}{\\partial b_{j}}\\\\\n",
    "=&\\frac{\\partial C}{\\partial \\sigma_{j}}\\cdot \\frac{\\partial(w_{jk}a_{k} + b_{j})}{\\partial b_{j}}\\\\\n",
    "=&\\frac{\\partial}{\\partial \\sigma_{j}}\\left(-\\sum_{k}{y_{k}\\ln a_{k}}\\right)\\\\\n",
    "=&-\\sum_{k}{y_{k}\\frac{1}{a_{k}}}\\cdot \\frac{\\partial a_{k}}{\\partial \\sigma_{j}}\\\\\n",
    "=&-y_{j}\\frac{1}{a_{j}}\\cdot\\frac{\\partial a_{j}}{\\partial \\sigma_{j}} - \\sum_{k\\neq {j}}{y_{k}\\frac{1}{a_{k}}\\cdot \\frac{\\partial a_{k}}{\\partial \\sigma_{j}}}\\\\\n",
    "=& -y_{j}\\frac{1}{a_{j}}\\cdot a_{j}(1-a_{j}) - \\sum_{k\\neq {j}}{y_{k}\\frac{1}{a_{k}}\\cdot (-a_{j}a_{k}})\\\\\n",
    "=&-y_{j}+y_{j}a_{j} + \\sum_{k\\neq {j}}{y_{k}a_{j}}\\\\\n",
    "=&-y_{j} + a_{j}\\sum_{k}{y_{k}}\\\\\n",
    "=&a_{j}-y_{j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "最后的结果看起来简单了很多，最后，针对分类问题，我们给定的结果$y_{j}$类别是1，其他类别都是0,因此对于分类问题，这个梯度等于：\n",
    "$\n",
    "a_{j} -y_{j} = a_{j}-1\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 同理可得：\n",
    " $$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial C}{\\partial w_{jk}}\\\n",
    "=&a_{k}^{L-1}(a_{j}^{L}-y_{j})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述梯度公式可知，softmax函数配合log似然代价函数可以很好地训练ANN，不存在学习速度变慢的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "https://blog.csdn.net/u014313009/article/details/51045303\n",
    "https://blog.csdn.net/allenlzcoder/article/details/78591535\n",
    "https://blog.csdn.net/qian99/article/details/78046329"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
